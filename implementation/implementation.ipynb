{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# project ai: Easer\n",
    "\n",
    "by Michiel TÃ©blick and thibaut Van Goethem\n",
    "\n",
    "In this notebook we will look at the easer model proposed at https://dl.acm.org/doi/pdf/10.1145/3308558.3313710.\n",
    "\n",
    "This model will be applied to a dataset from foods.com which containes a bunch of recipes with user ratings/reactions on them.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import statistics as st"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading and preprocessing the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of interactions in the full dataset:  1132367\n",
      "amount of recipes in the full dataset:  231637\n",
      "amount of users in the full dataset:  226570\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('../folds/fold_0/train.csv')\n",
    "df_test = pd.read_csv('../folds/fold_0/test.csv')\n",
    "df_validate = pd.read_csv('../folds/fold_0/validate.csv')\n",
    "df = pd.concat([df_train, df_test, df_validate])\n",
    "df.reset_index()\n",
    "\n",
    "print(\"amount of interactions in the full dataset: \",len(df))\n",
    "print(\"amount of recipes in the full dataset: \",len(df.recipe_id.unique()))\n",
    "print(\"amount of users in the full dataset: \",len(df.user_id.unique()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set all ratings to 1 (even negative interactions are seen as interactions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "df.loc[:,'rating'] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below here are two ways to cut down on the amount of interactions that are used in this notebook\n",
    "- The first one randomly removes x% of the users,\n",
    "- The second one removes all user and recipes that have less than X amount of interaction containing them\n",
    "\n",
    "We opted for the second form as this is more representative of how the models should be used due to the lower amount if recipes but more reactions per recipe. Also the second choice is a deterministic way of removing data, which the first one is not.\n",
    "This does end up mostly giving slightly worse result compared to the first choice.\n",
    "\n",
    "The reason we need to remove data is because a matrix inversion is done, which can not be done in a smart way.\n",
    "Also the result of the inversion is not necessarily a sparse matrix so the full calculation needs to be done on dense matrices. This end up scaling O(n^3) in time complexity and O(n^2) for memory needed. n here is the amount of recipes.\n",
    "So running on the full dataset would require more than 200gb of ram which we do not have.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### rescaling the id's\n",
    "The recipes and users don't go from 0 to amount so if we were to put this in a matrix we would get empty columns and rows. This is not that handy so we reindex both the user_id and recipe_ids\n",
    "\n",
    "This is a step we must not forget when entering the data in the model, as we also need to remap our input data using the same remapping that was used here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "userSet = set(df['user_id'].to_list())\n",
    "user_transform_dict = dict(map(reversed, enumerate(userSet)))\n",
    "recipeSet = set(df['recipe_id'].to_list())\n",
    "recipe_transform_dict = dict(map(reversed, enumerate(recipeSet)))\n",
    "recipe_dict = dict(enumerate(recipeSet))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "keep_nan_user = [k for k, v in user_transform_dict.items() if pd.isnull(v)]\n",
    "keep_nan_recipe = [k for k, v in recipe_transform_dict.items() if pd.isnull(v)]\n",
    "\n",
    "\n",
    "def transform_id(dataframe):\n",
    "    tochange = dataframe['user_id']\n",
    "    dataframe['user_id'] = tochange.map(user_transform_dict).fillna(tochange.mask(tochange.isin(keep_nan_user)))\n",
    "\n",
    "    tochange = dataframe['recipe_id']\n",
    "    dataframe['recipe_id'] = tochange.map(recipe_transform_dict).fillna(tochange.mask(tochange.isin(keep_nan_recipe)))\n",
    "    return dataframe\n",
    "\n",
    "def open_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    transform_id(df)\n",
    "    df.loc[:,'rating'] = 1\n",
    "    df.drop('review', axis=1, inplace=True)\n",
    "    df.drop('date', axis=1, inplace=True)\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creation of the folds\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "k = 10\n",
    "folds = list()\n",
    "for directory in [\"../folds/fold_%d\" % i for i in range(k)]:\n",
    "    folds.append((directory + \"/test.csv\", directory + \"/train.csv\", directory + \"/validate.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creation model\n",
    "Here we define the models used for the experiments. Both the easer predictor and a populaliry predictor are created. the popularity predictor is used as a baseline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class popularity:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def train(self, data):\n",
    "        data = data.sort_values('count_user',ascending=False)\n",
    "        self.pop = data[data.columns[1]].to_numpy()\n",
    "    def predict(self):\n",
    "        return self.pop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Easer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, X_train, lambda_=1250):\n",
    "        #Code here is a modified version of the code provided in the paper\n",
    "        self.X = X_train\n",
    "\n",
    "        G = X_train.T.dot(X_train)\n",
    "        G = G.toarray()\n",
    "        diagIndices = np.diag_indices(G.shape[0])\n",
    "        G[diagIndices] += lambda_\n",
    "        P = scipy.linalg.inv(G)\n",
    "        div = -np.diag(P)\n",
    "        self.B = P / div\n",
    "        self.B[diagIndices] = 0\n",
    "\n",
    "        self.pred = self.X * self.B\n",
    "\n",
    "    def predicts(self, xu):\n",
    "        return xu * self.B"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## training models + k-fold validation\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done fold: 0\n",
      "training took :  114.88806819915771 s\n",
      "done fold: 1\n",
      "training took :  113.40782117843628 s\n",
      "done fold: 2\n",
      "training took :  112.22119212150574 s\n",
      "done fold: 3\n",
      "training took :  114.10429406166077 s\n",
      "done fold: 4\n",
      "training took :  111.17725205421448 s\n",
      "done fold: 5\n",
      "training took :  111.97335028648376 s\n",
      "done fold: 6\n",
      "training took :  107.35915303230286 s\n",
      "done fold: 7\n",
      "training took :  113.66571307182312 s\n",
      "done fold: 8\n",
      "training took :  110.96520709991455 s\n",
      "done fold: 9\n",
      "training took :  113.2748670578003 s\n"
     ]
    }
   ],
   "source": [
    "#Please enter the path here of where you will place the pickle files (with trailing /)\n",
    "data_path=\"../results_aiproject/\"\n",
    "for f_idx, fold_files in enumerate(folds):\n",
    "    start = time.time()\n",
    "    train_data = open_csv(fold_files[0])\n",
    "    ratings = train_data.rating\n",
    "    idx = (train_data.user_id, train_data.recipe_id)\n",
    "    #Here we have the user item matrix\n",
    "    X_train = sparse.csc_matrix((ratings, idx), shape=(len(df.user_id.unique()), len(df.recipe_id.unique())),\n",
    "                                dtype=float)\n",
    "    #train models\n",
    "    model_pop=popularity()\n",
    "    model_pop.train(train_data)\n",
    "    model = Easer()\n",
    "    model.train(X_train)\n",
    "    print(\"done fold:\",str(f_idx))\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"training took : \", end - start, \"s\")\n",
    "\n",
    "    #Dump data for later usage (note that the easer model files end up being pretty large (approx 5gb each))\n",
    "    # datafile = open(data_path+\"data_fold\" + str(f_idx) + \".pkl\", mode='wb')\n",
    "    # pickle.dump(fold_data, datafile)\n",
    "    modelfile = open(data_path+\"model_fold\" + str(f_idx) + \".pkl\", mode='wb')\n",
    "    modelpopfile = open(data_path+\"model_pop_fold\" + str(f_idx) + \".pkl\", mode='wb')\n",
    "    pickle.dump(model, modelfile)\n",
    "    pickle.dump(model_pop, modelpopfile)\n",
    "    # datafile.close()\n",
    "    modelfile.close()\n",
    "    modelpopfile.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation results of the folds\n",
    "\n",
    "Here we use recall@20, recal@50 and ndcg@100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "K = 20\n",
    "K2 = 50"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easer fold: 0, recall@20 = 0.043204387135022976\n",
      "easer fold: 0, recall@50 = 0.07756533768094462\n",
      "easer fold: 0, ndcg@100 = 0.030759881131819704\n",
      "\n",
      "easer fold: 1, recall@20 = 0.04449000765791359\n",
      "easer fold: 1, recall@50 = 0.0783083421852227\n",
      "easer fold: 1, ndcg@100 = 0.03136378175091975\n",
      "\n",
      "easer fold: 2, recall@20 = 0.044020651663743485\n",
      "easer fold: 2, recall@50 = 0.07633210642029595\n",
      "easer fold: 2, ndcg@100 = 0.03070206895817723\n",
      "\n",
      "easer fold: 3, recall@20 = 0.044786443022652604\n",
      "easer fold: 3, recall@50 = 0.07583804747906425\n",
      "easer fold: 3, ndcg@100 = 0.030974543864755493\n",
      "\n",
      "easer fold: 4, recall@20 = 0.0443170870284825\n",
      "easer fold: 4, recall@50 = 0.07648032410266545\n",
      "easer fold: 4, ndcg@100 = 0.030936003172855706\n",
      "\n",
      "easer fold: 5, recall@20 = 0.04416886934611299\n",
      "easer fold: 5, recall@50 = 0.07564042390257157\n",
      "easer fold: 5, ndcg@100 = 0.030968373260119326\n",
      "\n",
      "easer fold: 6, recall@20 = 0.04426768113435933\n",
      "easer fold: 6, recall@50 = 0.07522047380252464\n",
      "easer fold: 6, ndcg@100 = 0.030627769331440992\n",
      "\n",
      "easer fold: 7, recall@20 = 0.04313134556952644\n",
      "easer fold: 7, recall@50 = 0.07522047380252464\n",
      "easer fold: 7, ndcg@100 = 0.030032203608860407\n",
      "\n",
      "easer fold: 8, recall@20 = 0.0451075813344532\n",
      "easer fold: 8, recall@50 = 0.07741903609100566\n",
      "easer fold: 8, ndcg@100 = 0.03119167015292627\n",
      "\n",
      "easer fold: 9, recall@20 = 0.04619451100516292\n",
      "easer fold: 9, recall@50 = 0.07892591586176231\n",
      "easer fold: 9, ndcg@100 = 0.03177313086905056\n",
      "\n",
      "mean recall@20 over 10 folds:  0.044368856489743\n",
      "mean recall@50 over 10 folds:  0.07669504813285818\n",
      "mean ndcg@100 over 10 folds:  0.030932942610092544\n",
      "\n",
      "standard deviation recall@20 over 10 folds:  0.0008443556755148848\n",
      "standard deviation recall@50 over 10 folds:  0.0012342732833226016\n",
      "standard deviation ndcg@100 over 10 folds:  0.0004414961354677954\n"
     ]
    }
   ],
   "source": [
    "result_list_K = list()\n",
    "result_list_K2 = list()\n",
    "result_ndcg = list()\n",
    "for i in range(k):\n",
    "\n",
    "    #Evaluate recall@k\n",
    "    #Do elementwise multiplication of top K predicts and true interactions\n",
    "\n",
    "    data = pickle.load(open(data_path+\"data_fold\"+str(i)+\".pkl\", mode='rb'))\n",
    "    model = pickle.load(open(data_path+\"model_fold\"+str(i)+\".pkl\", mode='rb'))\n",
    "\n",
    "    test_data = data[1]\n",
    "    predict_data = data[0]\n",
    "    total = len(test_data)\n",
    "\n",
    "    ratings = predict_data.rating\n",
    "    idx = (predict_data.user_id, predict_data.recipe_id)\n",
    "    X_train = sparse.csc_matrix((ratings, idx), shape=(len(df.user_id.unique()), len(df.recipe_id.unique())), dtype=float)\n",
    "    y_pred = model.pred\n",
    "\n",
    "    ratings_test = test_data.rating\n",
    "    idx_test = (test_data.user_id, test_data.recipe_id)\n",
    "    X_test = sparse.csc_matrix((ratings_test, idx_test), shape=(len(df.user_id.unique()), len(df.recipe_id.unique())), dtype=np.single)\n",
    "\n",
    "    interacted_recipes = (X_train == 1).toarray()\n",
    "    y_pred[interacted_recipes] = -100000\n",
    "    idx_top_scores = (-y_pred).argsort()[:,:100]\n",
    "    dense_X_test = X_test.toarray()\n",
    "\n",
    "    correct_K = 0\n",
    "    correct_K2 = 0\n",
    "    ndcg = 0\n",
    "\n",
    "    for idx, row in enumerate(idx_top_scores):\n",
    "        for rank, index in enumerate(row):\n",
    "            if dense_X_test[idx][index] == 1:\n",
    "                if rank < K:\n",
    "                    correct_K += 1\n",
    "                if rank < K2:\n",
    "                    correct_K2 += 1\n",
    "                ndcg += 1/(math.log2(rank+2))\n",
    "\n",
    "    result_list_K.append(correct_K / total)\n",
    "    result_list_K2.append(correct_K2 / total)\n",
    "    result_ndcg.append(ndcg / total)\n",
    "\n",
    "    print(\"easer fold: %s, recall@%s = %s\" % (str(i), str(K), str(correct_K / total)))\n",
    "    print(\"easer fold: %s, recall@%s = %s\" % (str(i), str(K2), str(correct_K2 / total)))\n",
    "    print(\"easer fold: %s, ndcg@%s = %s\" % (str(i), 100, str(ndcg / total)), end=\"\\n\\n\")\n",
    "\n",
    "print(\"mean recall@%s over 10 folds: \" % str(K), str(st.mean(result_list_K)))\n",
    "print(\"mean recall@%s over 10 folds: \" % str(K2), str(st.mean(result_list_K2)))\n",
    "print(\"mean ndcg@%s over 10 folds: \" % str(100), str(st.mean(result_ndcg)), end=\"\\n\\n\")\n",
    "print(\"standard deviation recall@%s over 10 folds: \" % str(K), str(st.pstdev(result_list_K)))\n",
    "print(\"standard deviation recall@%s over 10 folds: \" % str(K2), str(st.pstdev(result_list_K2)))\n",
    "print(\"standard deviation ndcg@%s over 10 folds: \" % str(100), str(st.pstdev(result_ndcg)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popularity fold: 0, recall@20 = 0.002396126673583321\n",
      "popularity fold: 0, recall@50 = 0.0036559458524776443\n",
      "popularity fold: 0, ndcg@100 = 0.0016267585221498146\n",
      "\n",
      "popularity fold: 1, recall@20 = 0.0010375237765865469\n",
      "popularity fold: 1, recall@50 = 0.004866480571132136\n",
      "popularity fold: 1, ndcg@100 = 0.0016943386433841108\n",
      "\n",
      "popularity fold: 2, recall@20 = 0.0008151972530322868\n",
      "popularity fold: 2, recall@50 = 0.002272671129665769\n",
      "popularity fold: 2, ndcg@100 = 0.0013136193129889022\n",
      "\n",
      "popularity fold: 3, recall@20 = 0.0008399002000938712\n",
      "popularity fold: 3, recall@50 = 0.0017292062943109112\n",
      "popularity fold: 3, ndcg@100 = 0.0007970803395989333\n",
      "\n",
      "popularity fold: 4, recall@20 = 0.0013586620883871447\n",
      "popularity fold: 4, recall@50 = 0.0026679182826511204\n",
      "popularity fold: 4, ndcg@100 = 0.001116874704603834\n",
      "\n",
      "popularity fold: 5, recall@20 = 0.0012104444060176379\n",
      "popularity fold: 5, recall@50 = 0.002371482917912107\n",
      "popularity fold: 5, ndcg@100 = 0.0011670027158484017\n",
      "\n",
      "popularity fold: 6, recall@20 = 0.0008893060942170401\n",
      "popularity fold: 6, recall@50 = 0.0039277685827919274\n",
      "popularity fold: 6, ndcg@100 = 0.0015117882757813064\n",
      "\n",
      "popularity fold: 7, recall@20 = 0.0010869296707097156\n",
      "popularity fold: 7, recall@50 = 0.002149156394357847\n",
      "popularity fold: 7, ndcg@100 = 0.0010361767570761702\n",
      "\n",
      "popularity fold: 8, recall@20 = 0.0012845532472023913\n",
      "popularity fold: 8, recall@50 = 0.00466885699463946\n",
      "popularity fold: 8, ndcg@100 = 0.0015385407850420905\n",
      "\n",
      "popularity fold: 9, recall@20 = 0.0028408389120822115\n",
      "popularity fold: 9, recall@50 = 0.005706380771226007\n",
      "popularity fold: 9, ndcg@100 = 0.0018280857840796557\n",
      "\n",
      "mean recall@20 over 10 folds:  0.0013759482321912167\n",
      "mean recall@50 over 10 folds:  0.003401586779116493\n",
      "mean ndcg@100 over 10 folds:  0.001363026584055322\n",
      "\n",
      "standard deviation recall@20 over 10 folds:  0.0006526396166083474\n",
      "standard deviation recall@50 over 10 folds:  0.0012901473218821144\n",
      "standard deviation ndcg@100 over 10 folds:  0.0003125892984632328\n"
     ]
    }
   ],
   "source": [
    "#recall score for popularity\n",
    "result_list_pop_K=list()\n",
    "result_list_pop_K2=list()\n",
    "result_list_pop_ndcg=list()\n",
    "for i in range(k):\n",
    "    data = pickle.load(open(data_path+\"data_fold\"+str(i)+\".pkl\", mode='rb'))\n",
    "    model = pickle.load(open(data_path+\"model_pop_fold\"+str(i)+\".pkl\", mode='rb'))\n",
    "    test_data = data[1]\n",
    "    predict_data = data[0]\n",
    "    pop=model.predict()\n",
    "    total = 0\n",
    "    correct_K = 0\n",
    "    correct_K2 = 0\n",
    "    ndcg = 0\n",
    "    for idx, interaction in test_data.iterrows():\n",
    "        user = interaction['user_id']\n",
    "        user_data = predict_data.loc[(predict_data['user_id'] == user)]\n",
    "        already_interacted_recipes = user_data[user_data.columns[1]].to_numpy()\n",
    "        newpop = pop[:150]\n",
    "        newpop = newpop[~np.in1d(newpop,already_interacted_recipes)]\n",
    "        newpop_K = newpop[:K]\n",
    "        newpop_K2 = newpop[:K2]\n",
    "        newpop_ndcg = newpop[:100]\n",
    "        recipe = interaction['recipe_id']\n",
    "        if recipe in newpop_K:\n",
    "            correct_K += 1\n",
    "        if recipe in newpop_K2:\n",
    "            correct_K2 += 1\n",
    "        if recipe in newpop_ndcg:\n",
    "            ndcg += 1/(math.log2(np.where(newpop_ndcg == recipe)[0]+2))\n",
    "        total += 1\n",
    "    result_list_pop_K.append(correct_K / total)\n",
    "    result_list_pop_K2.append(correct_K2 / total)\n",
    "    result_list_pop_ndcg.append(ndcg / total)\n",
    "    print(\"popularity fold: %s, recall@%s = %s\" % (str(i),str(K), str(correct_K / total)))\n",
    "    print(\"popularity fold: %s, recall@%s = %s\" % (str(i),str(K2), str(correct_K2 / total)))\n",
    "    print(\"popularity fold: %s, ndcg@%s = %s\" % (str(i),str(100), str(ndcg / total)), end=\"\\n\\n\")\n",
    "\n",
    "print(\"mean recall@%s over 10 folds: \" % str(K), str(st.mean(result_list_pop_K)))\n",
    "print(\"mean recall@%s over 10 folds: \" % str(K2), str(st.mean(result_list_pop_K2)))\n",
    "print(\"mean ndcg@%s over 10 folds: \" % str(100), str(st.mean(result_list_pop_ndcg)), end=\"\\n\\n\")\n",
    "print(\"standard deviation recall@%s over 10 folds: \" % str(K), str(st.pstdev(result_list_pop_K)))\n",
    "print(\"standard deviation recall@%s over 10 folds: \" % str(K2), str(st.pstdev(result_list_pop_K2)))\n",
    "print(\"standard deviation ndcg@%s over 10 folds: \" % str(100), str(st.pstdev(result_list_pop_ndcg)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next section is a demonstration that selects a random user and makes a recommendation prediction for this user."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             name  \\\n76978              easy lemon dijon shrimp scampi   \n115988  just like loaded baked potatoes casserole   \n125954                   low carb breaded chicken   \n158185                           pepsi pork roast   \n188871                   slow cooker chicken stew   \n\n                                              ingredients  \n76978   ['large shrimp', 'fresh garlic', 'butter', 'dr...  \n115988  ['cauliflower', 'sour cream', 'shredded chedda...  \n125954  ['boneless skinless chicken breasts', 'egg', '...  \n158185  ['pork shoulder butt', 'pepsi', 'cream of mush...  \n188871  ['boneless skinless chicken breast', 'boneless...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>ingredients</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>76978</th>\n      <td>easy lemon dijon shrimp scampi</td>\n      <td>['large shrimp', 'fresh garlic', 'butter', 'dr...</td>\n    </tr>\n    <tr>\n      <th>115988</th>\n      <td>just like loaded baked potatoes casserole</td>\n      <td>['cauliflower', 'sour cream', 'shredded chedda...</td>\n    </tr>\n    <tr>\n      <th>125954</th>\n      <td>low carb breaded chicken</td>\n      <td>['boneless skinless chicken breasts', 'egg', '...</td>\n    </tr>\n    <tr>\n      <th>158185</th>\n      <td>pepsi pork roast</td>\n      <td>['pork shoulder butt', 'pepsi', 'cream of mush...</td>\n    </tr>\n    <tr>\n      <th>188871</th>\n      <td>slow cooker chicken stew</td>\n      <td>['boneless skinless chicken breast', 'boneless...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                                                     name  \\\n62431                          creamy cajun chicken pasta   \n66687   crock pot chicken with black beans   cream cheese   \n98463                         grilled cheese  diner style   \n114444                             japanese mum s chicken   \n115303                   jo mama s world famous spaghetti   \n126146                                     low carb pizza   \n126216                                 low carb taco bake   \n163319                                 pork chops yum yum   \n165494                                       poverty meal   \n213826                         to die for crock pot roast   \n\n                                              ingredients  \n62431   ['boneless skinless chicken breast halves', 'l...  \n66687   ['boneless chicken breasts', 'black beans', 'c...  \n98463          ['bread', 'american cheese', 'mayonnaise']  \n114444  ['chicken drumsticks', 'water', 'balsamic vine...  \n115303  ['italian sausage', 'onion', 'garlic cloves', ...  \n126146  ['cream cheese', 'eggs', 'heavy cream', 'parme...  \n126216  ['cream cheese', 'eggs', 'heavy cream', 'taco ...  \n163319  ['pork chops', 'chicken broth', 'honey', 'soy ...  \n165494  ['onion', 'garlic cloves', 'olive oil', 'groun...  \n213826  ['beef roast', 'brown gravy mix', 'dried itali...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>ingredients</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>62431</th>\n      <td>creamy cajun chicken pasta</td>\n      <td>['boneless skinless chicken breast halves', 'l...</td>\n    </tr>\n    <tr>\n      <th>66687</th>\n      <td>crock pot chicken with black beans   cream cheese</td>\n      <td>['boneless chicken breasts', 'black beans', 'c...</td>\n    </tr>\n    <tr>\n      <th>98463</th>\n      <td>grilled cheese  diner style</td>\n      <td>['bread', 'american cheese', 'mayonnaise']</td>\n    </tr>\n    <tr>\n      <th>114444</th>\n      <td>japanese mum s chicken</td>\n      <td>['chicken drumsticks', 'water', 'balsamic vine...</td>\n    </tr>\n    <tr>\n      <th>115303</th>\n      <td>jo mama s world famous spaghetti</td>\n      <td>['italian sausage', 'onion', 'garlic cloves', ...</td>\n    </tr>\n    <tr>\n      <th>126146</th>\n      <td>low carb pizza</td>\n      <td>['cream cheese', 'eggs', 'heavy cream', 'parme...</td>\n    </tr>\n    <tr>\n      <th>126216</th>\n      <td>low carb taco bake</td>\n      <td>['cream cheese', 'eggs', 'heavy cream', 'taco ...</td>\n    </tr>\n    <tr>\n      <th>163319</th>\n      <td>pork chops yum yum</td>\n      <td>['pork chops', 'chicken broth', 'honey', 'soy ...</td>\n    </tr>\n    <tr>\n      <th>165494</th>\n      <td>poverty meal</td>\n      <td>['onion', 'garlic cloves', 'olive oil', 'groun...</td>\n    </tr>\n    <tr>\n      <th>213826</th>\n      <td>to die for crock pot roast</td>\n      <td>['beef roast', 'brown gravy mix', 'dried itali...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "# read recipe data and load pre-trained model\n",
    "df_recipes = pd.read_csv('../data/RAW_recipes.csv')\n",
    "df_recipes.drop(['minutes', 'contributor_id', 'submitted', 'tags',\n",
    "                 'nutrition', 'n_steps', 'steps', 'description', 'n_ingredients'], axis=1, inplace=True)\n",
    "data = pickle.load(open(data_path+\"data_fold0.pkl\", mode='rb'))\n",
    "model = pickle.load(open(data_path+\"model_fold0.pkl\", mode='rb'))\n",
    "predict_data = data[0]\n",
    "ratings = predict_data.rating\n",
    "idx = (predict_data.user_id, predict_data.recipe_id)\n",
    "x_train = sparse.csc_matrix((ratings, idx), shape=(len(df.user_id.unique()), len(df.recipe_id.unique())), dtype=float)\n",
    "\n",
    "# get random user and make prediction\n",
    "random_user = x_train.getrow(random.randint(0, len(df.user_id.unique())))\n",
    "prediction = model.predicts(random_user)[0]\n",
    "interacted_recipes = []\n",
    "for recipe_id in random_user.indices:\n",
    "    interacted_recipes.append(recipe_dict[recipe_id])\n",
    "    prediction[recipe_id] = -100000\n",
    "\n",
    "\n",
    "top_index = (-prediction).argsort()[:10]\n",
    "recommended_recipes = []\n",
    "for recipe_id in top_index:\n",
    "    recommended_recipes.append(recipe_dict[recipe_id])\n",
    "\n",
    "# get interacted recipes and recommended recipes\n",
    "user_interactions = df_recipes[df_recipes['id'].isin(interacted_recipes)].drop('id', axis=1)\n",
    "user_recommendations = df_recipes[df_recipes['id'].isin(recommended_recipes)].drop('id', axis=1)\n",
    "\n",
    "display(user_interactions)\n",
    "display(user_recommendations)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}