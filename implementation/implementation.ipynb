{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# project ai: Easer\n",
    "\n",
    "by Michiel Téblick and thibaut Van Goethem\n",
    "\n",
    "In this notebook we will look at the easer model proposed at https://dl.acm.org/doi/pdf/10.1145/3308558.3313710.\n",
    "\n",
    "This model will be applied to a dataset from foods.com which containes a bunch of recipes with user ratings/reactions on them.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import statistics as st"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading and preprocessing the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of interactions in the full dataset:  733951\n",
      "amount of recipes in the full dataset:  80511\n",
      "amount of users in the full dataset:  32635\n"
     ]
    },
    {
     "data": {
      "text/plain": "         index  user_id  recipe_id        date  rating  \\\n0            0    56680      79222  2006-11-11       5   \n1            1   827374      79222  2010-11-29       3   \n2            2   462571     208980  2007-07-05       5   \n3            3   222139     208980  2007-09-08       5   \n4            4   423539     342209  2009-06-02       5   \n...        ...      ...        ...         ...     ...   \n733946  151908    96177     196735  2009-01-12       5   \n733947  151909   573325     196735  2010-09-08       5   \n733948  151910   203111     213546  2007-06-28       5   \n733949  151911    41468      82303  2006-09-01       5   \n733950  151912   207616      40514  2008-07-10       5   \n\n                                                   review  count_user  \\\n0       Oh, This was wonderful!  Had a soup and salad ...         174   \n1       We made this last night and really enjoyed it....          10   \n2       These were a snap to whip up and were fantasti...          87   \n3       I chose this recipe from Fall Pick A Chef.  Co...         446   \n4       These are lovely cookies not bland at all. \\r\\...          35   \n...                                                   ...         ...   \n733946  We just loved these tatters. Quick easy and ve...         563   \n733947  What a great, healthy, easy and yummy recipe!<...         880   \n733948  Very good potatoes!  I served them with fried ...         217   \n733949  WOW this was great. What I love the most is th...          30   \n733950  A quick and easy lunch for me using little bit...         149   \n\n        count_item  \n0               18  \n1               18  \n2                8  \n3                8  \n4                8  \n...            ...  \n733946          17  \n733947          17  \n733948           6  \n733949          13  \n733950           4  \n\n[733951 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>user_id</th>\n      <th>recipe_id</th>\n      <th>date</th>\n      <th>rating</th>\n      <th>review</th>\n      <th>count_user</th>\n      <th>count_item</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>56680</td>\n      <td>79222</td>\n      <td>2006-11-11</td>\n      <td>5</td>\n      <td>Oh, This was wonderful!  Had a soup and salad ...</td>\n      <td>174</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>827374</td>\n      <td>79222</td>\n      <td>2010-11-29</td>\n      <td>3</td>\n      <td>We made this last night and really enjoyed it....</td>\n      <td>10</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>462571</td>\n      <td>208980</td>\n      <td>2007-07-05</td>\n      <td>5</td>\n      <td>These were a snap to whip up and were fantasti...</td>\n      <td>87</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>222139</td>\n      <td>208980</td>\n      <td>2007-09-08</td>\n      <td>5</td>\n      <td>I chose this recipe from Fall Pick A Chef.  Co...</td>\n      <td>446</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>423539</td>\n      <td>342209</td>\n      <td>2009-06-02</td>\n      <td>5</td>\n      <td>These are lovely cookies not bland at all. \\r\\...</td>\n      <td>35</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>733946</th>\n      <td>151908</td>\n      <td>96177</td>\n      <td>196735</td>\n      <td>2009-01-12</td>\n      <td>5</td>\n      <td>We just loved these tatters. Quick easy and ve...</td>\n      <td>563</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>733947</th>\n      <td>151909</td>\n      <td>573325</td>\n      <td>196735</td>\n      <td>2010-09-08</td>\n      <td>5</td>\n      <td>What a great, healthy, easy and yummy recipe!&lt;...</td>\n      <td>880</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>733948</th>\n      <td>151910</td>\n      <td>203111</td>\n      <td>213546</td>\n      <td>2007-06-28</td>\n      <td>5</td>\n      <td>Very good potatoes!  I served them with fried ...</td>\n      <td>217</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>733949</th>\n      <td>151911</td>\n      <td>41468</td>\n      <td>82303</td>\n      <td>2006-09-01</td>\n      <td>5</td>\n      <td>WOW this was great. What I love the most is th...</td>\n      <td>30</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>733950</th>\n      <td>151912</td>\n      <td>207616</td>\n      <td>40514</td>\n      <td>2008-07-10</td>\n      <td>5</td>\n      <td>A quick and easy lunch for me using little bit...</td>\n      <td>149</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>733951 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_less_data = False\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('../folds/fold_0/train.csv')\n",
    "df_test = pd.read_csv('../folds/fold_0/test.csv')\n",
    "df_validate = pd.read_csv('../folds/fold_0/validate.csv')\n",
    "df = pd.concat([df_train, df_test, df_validate])\n",
    "#df.reset_index()\n",
    "\n",
    "print(\"amount of interactions in the full dataset: \",len(df))\n",
    "print(\"amount of recipes in the full dataset: \",len(df.recipe_id.unique()))\n",
    "print(\"amount of users in the full dataset: \",len(df.user_id.unique()))\n",
    "\n",
    "if use_less_data:\n",
    "    df = df[df['count_item'] >= 10]\n",
    "    print(\"amount of recipes in the smaller dataset: \",len(df.recipe_id.unique()))\n",
    "    print(\"amount of users in the smaller dataset: \",len(df.user_id.unique()))\n",
    "df.reset_index()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set all ratings to 1 (even negative interactions are seen as interactions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "df.loc[:,'rating'] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below here are two ways to cut down on the amount of interactions that are used in this notebook\n",
    "- The first one randomly removes x% of the users,\n",
    "- The second one removes all user and recipes that have less than X amount of interaction containing them\n",
    "\n",
    "We opted for the second form as this is more representative of how the models should be used due to the lower amount if recipes but more reactions per recipe. Also the second choice is a deterministic way of removing data, which the first one is not.\n",
    "This does end up mostly giving slightly worse result compared to the first choice.\n",
    "\n",
    "The reason we need to remove data is because a matrix inversion is done, which can not be done in a smart way.\n",
    "Also the result of the inversion is not necessarily a sparse matrix so the full calculation needs to be done on dense matrices. This end up scaling O(n^3) in time complexity and O(n^2) for memory needed. n here is the amount of recipes.\n",
    "So running on the full dataset would require more than 200gb of ram which we do not have.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### rescaling the id's\n",
    "The recipes and users don't go from 0 to amount so if we were to put this in a matrix we would get empty columns and rows. This is not that handy so we reindex both the user_id and recipe_ids\n",
    "\n",
    "This is a step we must not forget when entering the data in the model, as we also need to remap our input data using the same remapping that was used here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "userSet = set(df['user_id'].to_list())\n",
    "user_transform_dict = dict(map(reversed, enumerate(userSet)))\n",
    "recipeSet = set(df['recipe_id'].to_list())\n",
    "recipe_transform_dict = dict(map(reversed, enumerate(recipeSet)))\n",
    "recipe_dict = dict(enumerate(recipeSet))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "keep_nan_user = [k for k, v in user_transform_dict.items() if pd.isnull(v)]\n",
    "keep_nan_recipe = [k for k, v in recipe_transform_dict.items() if pd.isnull(v)]\n",
    "\n",
    "\n",
    "def transform_id(dataframe):\n",
    "    tochange = dataframe['user_id']\n",
    "    dataframe['user_id'] = tochange.map(user_transform_dict).fillna(tochange.mask(tochange.isin(keep_nan_user)))\n",
    "\n",
    "    tochange = dataframe['recipe_id']\n",
    "    dataframe['recipe_id'] = tochange.map(recipe_transform_dict).fillna(tochange.mask(tochange.isin(keep_nan_recipe)))\n",
    "    return dataframe\n",
    "\n",
    "def open_csv(filename, use_less_data=False):\n",
    "    df = pd.read_csv(filename)\n",
    "    if use_less_data:\n",
    "        df = df[df['count_item'] >= 10]\n",
    "    df = transform_id(df)\n",
    "    df.loc[:,'rating'] = 1\n",
    "    df.drop('review', axis=1, inplace=True)\n",
    "    #df.drop('date', axis=1, inplace=True)\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creation of the folds\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "k = 10\n",
    "folds = list()\n",
    "for directory in [\"../folds/fold_%d\" % i for i in range(k)]:\n",
    "    folds.append(( directory + \"/train.csv\", directory + \"/validate.csv\",directory + \"/test.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creation model\n",
    "Here we define the models used for the experiments. Both the easer predictor and a populaliry predictor are created. the popularity predictor is used as a baseline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def split_test(data_set):\n",
    "    ground_truth = data_set.sort_values('date').groupby('user_id').tail(1)\n",
    "    predict = pd.concat([data_set, ground_truth]).drop_duplicates(keep=False)\n",
    "    return predict, ground_truth\n",
    "\n",
    "def data_frame_to_matrix(dataframe):\n",
    "    ratings = dataframe.rating\n",
    "    idx = (dataframe.user_id, dataframe.recipe_id)\n",
    "    return sparse.csc_matrix((ratings, idx), shape=(len(df.user_id.unique()), len(df.recipe_id.unique())),\n",
    "                                dtype=float)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "class popularity:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def train(self, data):\n",
    "        data = data.sort_values('count_user',ascending=False)\n",
    "        self.pop = data[data.columns[1]].to_numpy()\n",
    "    def predict(self):\n",
    "        return self.pop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "class Easer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, X_train, lambda_=1250):\n",
    "        #Code here is a modified version of the code provided in the paper\n",
    "        #self.X = X_train\n",
    "\n",
    "        G = X_train.T.dot(X_train)\n",
    "        G = G.toarray()\n",
    "        diagIndices = np.diag_indices(G.shape[0])\n",
    "        G[diagIndices] += lambda_\n",
    "        diagIndices\n",
    "        P = scipy.linalg.inv(G)\n",
    "        del G\n",
    "        div = -np.diag(P)\n",
    "        self.B = P / div\n",
    "        self.B[diagIndices] = 0\n",
    "\n",
    "        #self.pred = self.X * self.B\n",
    "\n",
    "    def predict(self, xu):\n",
    "        return xu * self.B\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "K = 20\n",
    "K2 = 50\n",
    "\n",
    "def recal_easer(model, predict_data, test_data):\n",
    "    total = len(test_data)\n",
    "\n",
    "    X_train = data_frame_to_matrix(predict_data)\n",
    "    y_pred = model.predict(X_train)\n",
    "\n",
    "    X_test = data_frame_to_matrix(test_data)\n",
    "\n",
    "    interacted_recipes = (X_train == 1).toarray()\n",
    "    y_pred[interacted_recipes] = -100000\n",
    "    idx_top_scores = (-y_pred).argsort()[:,:100]\n",
    "    dense_X_test = X_test.toarray()\n",
    "\n",
    "    correct_K = 0\n",
    "    correct_K2 = 0\n",
    "    ndcg = 0\n",
    "\n",
    "    for idx, row in enumerate(idx_top_scores):\n",
    "        for rank, index in enumerate(row):\n",
    "            if dense_X_test[idx][index] == 1:\n",
    "                if rank < K:\n",
    "                    correct_K += 1\n",
    "                if rank < K2:\n",
    "                    correct_K2 += 1\n",
    "                ndcg += 1/(math.log2(rank+2))\n",
    "\n",
    "    # print(\"easer recall@%s = %s\" % (str(K), str(correct_K / total)))\n",
    "    # print(\"easer recall@%s = %s\" % (str(K2), str(correct_K2 / total)))\n",
    "    # print(\"easer ndcg@%s = %s\" % (100, str(ndcg / total)), end=\"\\n\\n\")\n",
    "\n",
    "    return correct_K/total, correct_K2/total, ndcg/total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## training models + evaluation\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took :  2.55002498626709 s\n",
      "training took :  2.5050790309906006 s\n",
      "training took :  2.48468017578125 s\n",
      "training took :  2.5029921531677246 s\n",
      "training took :  2.6092259883880615 s\n",
      "training took :  2.7099993228912354 s\n",
      "training took :  2.6064834594726562 s\n",
      "training took :  2.5574355125427246 s\n",
      "training took :  2.5564146041870117 s\n",
      "training took :  2.5100324153900146 s\n"
     ]
    },
    {
     "ename": "StatisticsError",
     "evalue": "mean requires at least one data point",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mStatisticsError\u001B[0m                           Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-40-c49c224d907f>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     41\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"training took : \"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mend\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"s\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 43\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"mean recall@%s over 10 folds: \"\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mK\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mst\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult_list_K\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     44\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"mean recall@%s over 10 folds: \"\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mK2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mst\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult_list_K2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"mean ndcg@%s over 10 folds: \"\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m100\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mst\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult_ndcg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mend\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"\\n\\n\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\statistics.py\u001B[0m in \u001B[0;36mmean\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m    313\u001B[0m     \u001B[0mn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    314\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mn\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 315\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mStatisticsError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'mean requires at least one data point'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    316\u001B[0m     \u001B[0mT\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtotal\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcount\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_sum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    317\u001B[0m     \u001B[1;32massert\u001B[0m \u001B[0mcount\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mStatisticsError\u001B[0m: mean requires at least one data point"
     ]
    }
   ],
   "source": [
    "#Please enter the path here of where you will place the pickle files (with trailing /)\n",
    "data_path=\"D:/results_aiproject_improvement/\"\n",
    "result_list_K = list()\n",
    "result_list_K2 = list()\n",
    "result_ndcg = list()\n",
    "\n",
    "for f_idx, fold_files in enumerate(folds):\n",
    "    start = time.time()\n",
    "    train_data = open_csv(fold_files[0], True)\n",
    "    #Here we have the user item matrix\n",
    "    X_train = data_frame_to_matrix(train_data)\n",
    "\n",
    "    #train models\n",
    "\n",
    "    model_pop=popularity()\n",
    "    model_pop.train(train_data)\n",
    "    modelpopfile = open(data_path+\"model_pop_fold\" + str(f_idx) + \".pkl\", mode='wb')\n",
    "    pickle.dump(model_pop, modelpopfile)\n",
    "    modelpopfile.close()\n",
    "    del model_pop\n",
    "\n",
    "    test_data = open_csv(fold_files[1], use_less_data)\n",
    "\n",
    "    # model = Easer()\n",
    "    # model.train(X_train, lambda_=1250)\n",
    "    interactions, ground_truth = split_test(test_data)\n",
    "\n",
    "    # recall20, recall50, ndcg = recal_easer(model, interactions, ground_truth)\n",
    "\n",
    "    # result_list_K.append(recall20)\n",
    "    # result_list_K2.append(recall50)\n",
    "    # result_ndcg.append(ndcg)\n",
    "    #\n",
    "    # print(\"done fold:\",str(f_idx))\n",
    "    #\n",
    "    # print(\"easer fold: %s, recall@%s = %s\" % (str(f_idx), str(K), recall20))\n",
    "    # print(\"easer fold: %s, recall@%s = %s\" % (str(f_idx), str(K2), recall50))\n",
    "    # print(\"easer fold: %s, ndcg@%s = %s\" % (str(f_idx), 100, ndcg), end=\"\\n\\n\")\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"training took : \", end - start, \"s\")\n",
    "\n",
    "print(\"mean recall@%s over 10 folds: \" % str(K), str(st.mean(result_list_K)))\n",
    "print(\"mean recall@%s over 10 folds: \" % str(K2), str(st.mean(result_list_K2)))\n",
    "print(\"mean ndcg@%s over 10 folds: \" % str(100), str(st.mean(result_ndcg)), end=\"\\n\\n\")\n",
    "print(\"standard deviation recall@%s over 10 folds: \" % str(K), str(st.pstdev(result_list_K)))\n",
    "print(\"standard deviation recall@%s over 10 folds: \" % str(K2), str(st.pstdev(result_list_K2)))\n",
    "print(\"standard deviation ndcg@%s over 10 folds: \" % str(100), str(st.pstdev(result_ndcg)))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation results of the folds\n",
    "\n",
    "Here we use recall@20, recal@50 and ndcg@100\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popularity fold: 0, recall@5,10,20 = 0.0,0.00031318509238960227,0.0006263701847792045\n",
      "popularity fold: 0, ndcg@5,10,20 = 0.0,0.0,0.0\n",
      "\n",
      "popularity fold: 1, recall@5,10,20 = 0.0003136762860727729,0.0006273525721455458,0.0006273525721455458\n",
      "popularity fold: 1, ndcg@5,10,20 = 0.00015683814303638644,0.00015683814303638644,0.00015683814303638644\n",
      "\n",
      "popularity fold: 2, recall@5,10,20 = 0.0009439899307740718,0.0009439899307740718,0.0028319697923222154\n",
      "popularity fold: 2, ndcg@5,10,20 = 0.0004065543342417178,0.0004065543342417178,0.0004065543342417178\n",
      "\n",
      "popularity fold: 3, recall@5,10,20 = 0.0,0.0006279434850863422,0.004081632653061225\n",
      "popularity fold: 3, ndcg@5,10,20 = 0.0,0.0,0.0\n",
      "\n",
      "popularity fold: 4, recall@5,10,20 = 0.0006275494195167869,0.0006275494195167869,0.0009413241292751804\n",
      "popularity fold: 4, ndcg@5,10,20 = 0.00025652003931846086,0.00025652003931846086,0.00025652003931846086\n",
      "\n",
      "popularity fold: 5, recall@5,10,20 = 0.0,0.0,0.0\n",
      "popularity fold: 5, ndcg@5,10,20 = 0.0,0.0,0.0\n",
      "\n",
      "popularity fold: 6, recall@5,10,20 = 0.0,0.0,0.0\n",
      "popularity fold: 6, ndcg@5,10,20 = 0.0,0.0,0.0\n",
      "\n",
      "popularity fold: 7, recall@5,10,20 = 0.0,0.0,0.0025031289111389237\n",
      "popularity fold: 7, ndcg@5,10,20 = 0.0,0.0,0.0\n",
      "\n",
      "popularity fold: 8, recall@5,10,20 = 0.0025109855618330196,0.0025109855618330196,0.003138731952291274\n",
      "popularity fold: 8, ndcg@5,10,20 = 0.0016590014191552873,0.0016590014191552873,0.0016590014191552873\n",
      "\n",
      "popularity fold: 9, recall@5,10,20 = 0.0009398496240601503,0.0009398496240601503,0.0009398496240601503\n",
      "popularity fold: 9, ndcg@5,10,20 = 0.0003773127106962645,0.0003773127106962645,0.0003773127106962645\n",
      "\n",
      "mean recall@5,10,20 over 10 folds: 0.0005336050822256802,0.0006590855685805519,0.001569035981907372\n",
      "mean ndcg@5,10,20 over 10 folds: 0.0002856226646448117,0.0002856226646448117,0.0002856226646448117\n",
      "\n",
      "standard deviation recall@5,10,20 over 10 folds: 0.0007565943461057744,0.0007081901140331025,0.001368709275136039\n",
      "standard deviation ndcg@5,10,20 over 10 folds: 0.00048317022726964125,0.00048317022726964125,0.00048317022726964125\n"
     ]
    }
   ],
   "source": [
    "#recall score for popularity\n",
    "result_list_pop_K5=list()\n",
    "result_list_pop_K10=list()\n",
    "result_list_pop_K20=list()\n",
    "result_list_pop_ndcg5=list()\n",
    "result_list_pop_ndcg10=list()\n",
    "result_list_pop_ndcg20=list()\n",
    "for i in range(k):\n",
    "    test_data = open_csv(folds[i][2], True)\n",
    "    predict_data, ground_truth = split_test(test_data)\n",
    "    model = pickle.load(open(data_path+\"model_pop_fold\"+str(i)+\".pkl\", mode='rb'))\n",
    "\n",
    "    pop=model.predict()\n",
    "    total = 0\n",
    "    correct_K5 = 0\n",
    "    correct_K10 = 0\n",
    "    correct_K20 = 0\n",
    "    ndcg5 = 0\n",
    "    ndcg10 = 0\n",
    "    ndcg20 = 0\n",
    "    for idx, interaction in ground_truth.iterrows():\n",
    "        user = interaction['user_id']\n",
    "        user_data = predict_data.loc[(predict_data['user_id'] == user)]\n",
    "        already_interacted_recipes = user_data[user_data.columns[1]].to_numpy()\n",
    "        newpop = pop[:150]\n",
    "        newpop = newpop[~np.in1d(newpop,already_interacted_recipes)]\n",
    "        newpop_K5 = newpop[:5]\n",
    "        newpop_K10 = newpop[:10]\n",
    "        newpop_K20 = newpop[:20]\n",
    "        # newpop_ndcg5 = newpop[:5]\n",
    "        # newpop_ndcg10 = newpop[:10]\n",
    "        # newpop_ndcg20 = newpop[:20]\n",
    "        recipe = interaction['recipe_id']\n",
    "        if recipe in newpop_K5:\n",
    "            correct_K5 += 1\n",
    "        if recipe in newpop_K10:\n",
    "            correct_K10 += 1\n",
    "        if recipe in newpop_K20:\n",
    "            correct_K20 += 1\n",
    "\n",
    "        if recipe in newpop_K5:\n",
    "            ndcg5 += 1/(math.log2(np.where(newpop_K5 == recipe)[0]+2))\n",
    "        if recipe in newpop_K5:\n",
    "            ndcg10 += 1/(math.log2(np.where(newpop_K10 == recipe)[0]+2))\n",
    "        if recipe in newpop_K5:\n",
    "            ndcg20 += 1/(math.log2(np.where(newpop_K20 == recipe)[0]+2))\n",
    "        total += 1\n",
    "    result_list_pop_K5.append(correct_K5 / total)\n",
    "    result_list_pop_K10.append(correct_K10 / total)\n",
    "    result_list_pop_K20.append(correct_K20 / total)\n",
    "    result_list_pop_ndcg5.append(ndcg5 / total)\n",
    "    result_list_pop_ndcg10.append(ndcg10 / total)\n",
    "    result_list_pop_ndcg20.append(ndcg20 / total)\n",
    "    print(\"popularity fold: %s, recall@5,10,20 = %s,%s,%s\" % (str(i), str(correct_K5 / total),str(correct_K10 / total),str(correct_K20 / total)))\n",
    "    print(\"popularity fold: %s, ndcg@5,10,20 = %s,%s,%s\" % (str(i),str(ndcg5 / total),str(ndcg10 / total),str(ndcg20 / total)), end=\"\\n\\n\")\n",
    "\n",
    "print(\"mean recall@5,10,20 over 10 folds: %s,%s,%s\" % (str(st.mean(result_list_pop_K5)),str(st.mean(result_list_pop_K10)),str(st.mean(result_list_pop_K20))))\n",
    "print(\"mean ndcg@5,10,20 over 10 folds: %s,%s,%s\" % (str(st.mean(result_list_pop_ndcg5)),str(st.mean(result_list_pop_ndcg10)),str(st.mean(result_list_pop_ndcg20))), end=\"\\n\\n\")\n",
    "print(\"standard deviation recall@5,10,20 over 10 folds: %s,%s,%s\" %(str(st.pstdev(result_list_pop_K5)), str(st.pstdev(result_list_pop_K10)), str(st.pstdev(result_list_pop_K20))))\n",
    "print(\"standard deviation ndcg@5,10,20 over 10 folds: %s,%s,%s\" % (str(st.pstdev(result_list_pop_ndcg5)),str(st.pstdev(result_list_pop_ndcg10)),str(st.pstdev(result_list_pop_ndcg20))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next section is a demonstration that selects a random user and makes a recommendation prediction for this user."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import random\n",
    "# # read recipe data and load pre-trained model\n",
    "# df_recipes = pd.read_csv('../data/RAW_recipes.csv')\n",
    "# df_recipes.drop(['minutes', 'contributor_id', 'submitted', 'tags',\n",
    "#                  'nutrition', 'n_steps', 'steps', 'description', 'n_ingredients'], axis=1, inplace=True)\n",
    "# data = pickle.load(open(data_path+\"data_fold0.pkl\", mode='rb'))\n",
    "# model = pickle.load(open(data_path+\"model_fold0.pkl\", mode='rb'))\n",
    "# predict_data = data[0]\n",
    "# ratings = predict_data.rating\n",
    "# idx = (predict_data.user_id, predict_data.recipe_id)\n",
    "# x_train = sparse.csc_matrix((ratings, idx), shape=(len(df.user_id.unique()), len(df.recipe_id.unique())), dtype=float)\n",
    "#\n",
    "# # get random user and make prediction\n",
    "# random_user = x_train.getrow(random.randint(0, len(df.user_id.unique())))\n",
    "# prediction = model.predict(random_user)[0]\n",
    "# interacted_recipes = []\n",
    "# for recipe_id in random_user.indices:\n",
    "#     interacted_recipes.append(recipe_dict[recipe_id])\n",
    "#     prediction[recipe_id] = -100000\n",
    "#\n",
    "#\n",
    "# top_index = (-prediction).argsort()[:10]\n",
    "# recommended_recipes = []\n",
    "# for recipe_id in top_index:\n",
    "#     recommended_recipes.append(recipe_dict[recipe_id])\n",
    "#\n",
    "# # get interacted recipes and recommended recipes\n",
    "# user_interactions = df_recipes[df_recipes['id'].isin(interacted_recipes)].drop('id', axis=1)\n",
    "# user_recommendations = df_recipes[df_recipes['id'].isin(recommended_recipes)].drop('id', axis=1)\n",
    "#\n",
    "# display(user_interactions)\n",
    "# display(user_recommendations)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}