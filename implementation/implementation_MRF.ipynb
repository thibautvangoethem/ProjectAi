{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# project ai: Easer\n",
    "\n",
    "by Michiel Téblick and thibaut Van Goethem\n",
    "\n",
    "In this notebook we will look at a sparse approximation of the easer model.\n",
    "This implementation is based on the paper Markov random fields for collaborative filtering\n",
    "\n",
    "This model will be applied to a dataset from foods.com which containes a bunch of recipes with user ratings/reactions on them.\n",
    "Preprocessing and fold splitting is done ahead of time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import bottleneck as bn\n",
    "import time\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import statistics as st\n",
    "\n",
    "## Reading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of interactions in the full dataset:  733951\n",
      "amount of recipes in the full dataset:  80511\n",
      "amount of users in the full dataset:  32635\n"
     ]
    },
    {
     "data": {
      "text/plain": "         index  user_id  recipe_id        date  rating  \\\n0            0    56680      79222  2006-11-11       5   \n1            1   827374      79222  2010-11-29       3   \n2            2   462571     208980  2007-07-05       5   \n3            3   222139     208980  2007-09-08       5   \n4            4   423539     342209  2009-06-02       5   \n...        ...      ...        ...         ...     ...   \n733946  151908    96177     196735  2009-01-12       5   \n733947  151909   573325     196735  2010-09-08       5   \n733948  151910   203111     213546  2007-06-28       5   \n733949  151911    41468      82303  2006-09-01       5   \n733950  151912   207616      40514  2008-07-10       5   \n\n                                                   review  count_user  \\\n0       Oh, This was wonderful!  Had a soup and salad ...         174   \n1       We made this last night and really enjoyed it....          10   \n2       These were a snap to whip up and were fantasti...          87   \n3       I chose this recipe from Fall Pick A Chef.  Co...         446   \n4       These are lovely cookies not bland at all. \\r\\...          35   \n...                                                   ...         ...   \n733946  We just loved these tatters. Quick easy and ve...         563   \n733947  What a great, healthy, easy and yummy recipe!<...         880   \n733948  Very good potatoes!  I served them with fried ...         217   \n733949  WOW this was great. What I love the most is th...          30   \n733950  A quick and easy lunch for me using little bit...         149   \n\n        count_item  \n0               18  \n1               18  \n2                8  \n3                8  \n4                8  \n...            ...  \n733946          17  \n733947          17  \n733948           6  \n733949          13  \n733950           4  \n\n[733951 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>user_id</th>\n      <th>recipe_id</th>\n      <th>date</th>\n      <th>rating</th>\n      <th>review</th>\n      <th>count_user</th>\n      <th>count_item</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>56680</td>\n      <td>79222</td>\n      <td>2006-11-11</td>\n      <td>5</td>\n      <td>Oh, This was wonderful!  Had a soup and salad ...</td>\n      <td>174</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>827374</td>\n      <td>79222</td>\n      <td>2010-11-29</td>\n      <td>3</td>\n      <td>We made this last night and really enjoyed it....</td>\n      <td>10</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>462571</td>\n      <td>208980</td>\n      <td>2007-07-05</td>\n      <td>5</td>\n      <td>These were a snap to whip up and were fantasti...</td>\n      <td>87</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>222139</td>\n      <td>208980</td>\n      <td>2007-09-08</td>\n      <td>5</td>\n      <td>I chose this recipe from Fall Pick A Chef.  Co...</td>\n      <td>446</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>423539</td>\n      <td>342209</td>\n      <td>2009-06-02</td>\n      <td>5</td>\n      <td>These are lovely cookies not bland at all. \\r\\...</td>\n      <td>35</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>733946</th>\n      <td>151908</td>\n      <td>96177</td>\n      <td>196735</td>\n      <td>2009-01-12</td>\n      <td>5</td>\n      <td>We just loved these tatters. Quick easy and ve...</td>\n      <td>563</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>733947</th>\n      <td>151909</td>\n      <td>573325</td>\n      <td>196735</td>\n      <td>2010-09-08</td>\n      <td>5</td>\n      <td>What a great, healthy, easy and yummy recipe!&lt;...</td>\n      <td>880</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>733948</th>\n      <td>151910</td>\n      <td>203111</td>\n      <td>213546</td>\n      <td>2007-06-28</td>\n      <td>5</td>\n      <td>Very good potatoes!  I served them with fried ...</td>\n      <td>217</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>733949</th>\n      <td>151911</td>\n      <td>41468</td>\n      <td>82303</td>\n      <td>2006-09-01</td>\n      <td>5</td>\n      <td>WOW this was great. What I love the most is th...</td>\n      <td>30</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>733950</th>\n      <td>151912</td>\n      <td>207616</td>\n      <td>40514</td>\n      <td>2008-07-10</td>\n      <td>5</td>\n      <td>A quick and easy lunch for me using little bit...</td>\n      <td>149</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>733951 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_less_data = False  # set this to true for testing purposes\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('../folds/fold_0/train.csv')\n",
    "df_test = pd.read_csv('../folds/fold_0/test.csv')\n",
    "df_validate = pd.read_csv('../folds/fold_0/validate.csv')\n",
    "df = pd.concat([df_train, df_test, df_validate])\n",
    "\n",
    "print(\"amount of interactions in the full dataset: \",len(df))\n",
    "print(\"amount of recipes in the full dataset: \",len(df.recipe_id.unique()))\n",
    "print(\"amount of users in the full dataset: \",len(df.user_id.unique()))\n",
    "\n",
    "if use_less_data:\n",
    "    df = df[df['count_item'] >= 10]\n",
    "    print(\"amount of recipes in the smaller dataset: \",len(df.recipe_id.unique()))\n",
    "    print(\"amount of users in the smaller dataset: \",len(df.user_id.unique()))\n",
    "df.reset_index()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set all ratings to 1 (even negative interactions are seen as interactions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.loc[:,'rating'] = 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 76,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### rescaling the id's\n",
    "The recipes and users don't go from 0 to amount so if we were to put this in a matrix we would get empty columns and rows. This is not that handy so we reindex both the user_id and recipe_ids\n",
    "\n",
    "This is a step we must not forget when entering the data in the model, as we also need to remap our input data using the same remapping that was used here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "userSet = set(df['user_id'].to_list())\n",
    "user_transform_dict = dict(map(reversed, enumerate(userSet)))\n",
    "recipeSet = set(df['recipe_id'].to_list())\n",
    "recipe_transform_dict = dict(map(reversed, enumerate(recipeSet)))\n",
    "recipe_dict = dict(enumerate(recipeSet))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 77,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "keep_nan_user = [k for k, v in user_transform_dict.items() if pd.isnull(v)]\n",
    "keep_nan_recipe = [k for k, v in recipe_transform_dict.items() if pd.isnull(v)]\n",
    "\n",
    "\n",
    "def transform_id(dataframe):\n",
    "    tochange = dataframe['user_id']\n",
    "    dataframe['user_id'] = tochange.map(user_transform_dict).fillna(tochange.mask(tochange.isin(keep_nan_user)))\n",
    "\n",
    "    tochange = dataframe['recipe_id']\n",
    "    dataframe['recipe_id'] = tochange.map(recipe_transform_dict).fillna(tochange.mask(tochange.isin(keep_nan_recipe)))\n",
    "    return dataframe\n",
    "\n",
    "def open_csv(filename, use_less_data=False):\n",
    "    csv_df = pd.read_csv(filename)\n",
    "    if use_less_data:\n",
    "        csv_df = csv_df[csv_df['count_item'] >= 10]\n",
    "    csv_df = transform_id(csv_df)\n",
    "    csv_df.loc[:,'rating'] = 1\n",
    "    csv_df.drop('review', axis=1, inplace=True)\n",
    "    return csv_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creation of the folds\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "k = 10\n",
    "folds = list()\n",
    "for directory in [\"../folds/fold_%d\" % i for i in range(k)]:\n",
    "    folds.append((directory + \"/train.csv\", directory + \"/validate.csv\",directory + \"/test.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creation model\n",
    "Here we define the models used for the experiments. Both the easer predictor and a populaliry predictor are created. the popularity predictor is used as a baseline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "def split_test(data_set):\n",
    "    ground_truth = data_set.sort_values('date').groupby('user_id').tail(1)\n",
    "    predict = pd.concat([data_set, ground_truth]).drop_duplicates(keep=False)\n",
    "    return predict, ground_truth\n",
    "\n",
    "def data_frame_to_matrix(dataframe):\n",
    "    ratings = dataframe.rating\n",
    "    idx = (dataframe.user_id, dataframe.recipe_id)\n",
    "    return sparse.csc_matrix((ratings, idx), shape=(len(df.user_id.unique()), len(df.recipe_id.unique())),\n",
    "                                dtype=float)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creation model\n",
    "Here we define the models used for the experiments. Both the easer predictor and a populaliry predictor are created. the popularity predictor is used as a baseline\n",
    "This code is heavily inspired from: https://github.com/hasteck/MRF_NeurIPS_2019/blob/master/mrf_for_cf_NeurIPS_2019.ipynb\n",
    "The original code still used a lot of dense representations, we changed this to make more use out of sparse representations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "alpha = 0.75\n",
    "\n",
    "def precompute_sparse(train_data):\n",
    "    # this function standardizes the gram matrix, this helps us to later set a threshold value for calculating the sparsity pattern.\n",
    "    # If the vectors of the gram matrix our centered around 0 then gram matrix is proportional with the covariance matrix.\n",
    "    # we there for have to center our vectors around 0 and rescale them.\n",
    "    userCount = train_data.shape[0]\n",
    "    XtX= train_data.T.dot(train_data) #  Build the data gram matrix X^TX\n",
    "\n",
    "    # XtX\n",
    "    mu=XtX.diagonal() / userCount\n",
    "    variance_times_userCount = XtX.diagonal() - mu * mu * userCount\n",
    "\n",
    "    # standardizing the data-matrix XtX (if alpha=1, then XtX becomes the correlation matrix)\n",
    "    XtX -= scipy.sparse.csr_matrix(mu[:,None]) * scipy.sparse.csr_matrix(mu* userCount)\n",
    "    rescaling = np.power(variance_times_userCount, alpha / 2.0)\n",
    "    scaling = 1.0  / rescaling\n",
    "\n",
    "    XtX = scipy.sparse.diags(scaling) * XtX * scipy.sparse.diags(scaling)\n",
    "\n",
    "    XtXdiag = deepcopy(XtX.diagonal())\n",
    "    ii_diag = np.diag_indices(XtX.shape[0])\n",
    "\n",
    "    return XtX,XtXdiag,ii_diag,rescaling,scaling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "class sparseMRF:\n",
    "    def __init__(self,xtx,xtxdiag,ii,rescaling,scaling):\n",
    "        self.XtX=xtx\n",
    "        self.XtXdiag = xtxdiag\n",
    "        self.ii_diag = ii\n",
    "        self.threshold = 0.375\n",
    "        self.maxInColumn = 1000\n",
    "        # hyper-parameter r in the paper, which determines the trade-off between approximation-accuracy and training-time\n",
    "        self.rr = 0.5\n",
    "        self.L2reg = 1.0  # L2 norm regularization\n",
    "        self.scaling=scaling\n",
    "        self.rescaling=rescaling\n",
    "\n",
    "    def sparse_solution(self):\n",
    "\n",
    "        # sparsity pattern, see section 3.1 in the paper\n",
    "        # self.XtX[self.ii_diag] = self.XtXdiag\n",
    "        sparsity_pattern = self.calculate_sparsity_pattern()\n",
    "\n",
    "        # parameter-estimation, see section 3.2 in the paper\n",
    "        self.XtX[self.ii_diag] = self.XtXdiag+self.L2reg\n",
    "        BBsparse = self.sparse_parameter_estimation(sparsity_pattern)\n",
    "\n",
    "        return BBsparse\n",
    "\n",
    "\n",
    "    def calculate_sparsity_pattern(self):\n",
    "\n",
    "        # apply threshold\n",
    "        idx1, idx2, value = sparse.find(np.abs(self.XtX > self.threshold))\n",
    "        boolean_matrix = scipy.sparse.csc_matrix((value, (idx1, idx2)), dtype=np.float32)\n",
    "        sparsity_pattern = boolean_matrix.multiply(self.XtX)\n",
    "        sparsity_pattern.eliminate_zeros()\n",
    "\n",
    "        # enforce maxInColumn\n",
    "        countInColumns = sparsity_pattern.getnnz(axis=0) # count number of nonzero values in each column\n",
    "        columns = np.where(countInColumns > self.maxInColumn)[0] # get all column with more values than maxInColumn\n",
    "        for i in columns:\n",
    "            j= sparsity_pattern[:,i].nonzero()[0]\n",
    "            k = bn.argpartition(-np.abs(np.asarray(sparsity_pattern[j,i].todense()).flatten()), self.maxInColumn)[self.maxInColumn:]\n",
    "            sparsity_pattern[j[k], i] = 0.0 # set the lowest values to 0\n",
    "        sparsity_pattern.eliminate_zeros()\n",
    "\n",
    "        return sparsity_pattern\n",
    "\n",
    "    def sparse_parameter_estimation(self, sparsity_pattern):\n",
    "        # this implements section 3.2 in the paper\n",
    "\n",
    "        # list L in the paper, sorted by item-counts per column, ties broken by item-popularities as reflected by np.diag(XtX)\n",
    "        CountInColumns = sparsity_pattern.getnnz(axis=0)\n",
    "        # CountInColumns consists out of integers, so to break the tie on item-popularity we add a value\n",
    "        # between 0 and 1 to each value in CountInColumns. This value is proportional to the popularity.\n",
    "        sortedList=np.argsort(CountInColumns+ self.XtX.diagonal() /2.0/ np.max(self.XtX.diagonal()))[::-1]\n",
    "\n",
    "        # print(\"iterating through steps 1,2, and 4 in section 3.2 of the paper ...\")\n",
    "        todoIndicators=np.ones(CountInColumns.shape[0]) # structure to keep track of processed columns\n",
    "        blockList=[]   # list of blocks. Each block is a list of item-indices, to be processed in step 3 of the paper\n",
    "        for i in sortedList:\n",
    "            if todoIndicators[i]==1: # if column not yet processed\n",
    "                n, _, vals=sparse.find(sparsity_pattern[:,i])  # step 1 in paper: set n contains item i and its neighbors N\n",
    "                k=np.argsort(np.abs(vals))[::-1]\n",
    "                n=n[k] # sort nn\n",
    "                blockList.append(n) # list of items in the block, to be processed in step 3 below\n",
    "                # remove possibly several items from list L, as determined by parameter rr (r in the paper)\n",
    "                dd_count=max(1,int(np.ceil(len(n)*self.rr)))\n",
    "                dd=n[:dd_count] # set D, see step 2 in the paper\n",
    "                todoIndicators[dd]=0  # Set this column to done\n",
    "\n",
    "        # print(\"now step 3 in section 3.2 of the paper: iterating ...\")\n",
    "        # now the (possibly heavy) computations of step 3:\n",
    "        BBlist_ix1, BBlist_ix2, BBlist_val = [], [], []\n",
    "        denseXtX = self.XtX.toarray()\n",
    "\n",
    "        # calculate solution for each block matrix\n",
    "        for n in blockList:\n",
    "            BBblock=np.linalg.inv(denseXtX[np.ix_(n,n)])\n",
    "            BBblock/=-np.diag(BBblock)\n",
    "            # determine set D based on parameter rr (r in the paper)\n",
    "            dd_count=max(1,int(np.ceil(len(n)*self.rr)))\n",
    "            dd=n[:dd_count] # set D in paper\n",
    "            # store the solution regarding the items in D\n",
    "            blockix = np.meshgrid(dd,n)\n",
    "            BBlist_ix1.extend(blockix[1].flatten().tolist())\n",
    "            BBlist_ix2.extend(blockix[0].flatten().tolist())\n",
    "            BBlist_val.extend(BBblock[:,:dd_count].flatten().tolist())\n",
    "\n",
    "        del denseXtX\n",
    "\n",
    "        # Combine all block matrices\n",
    "        BBsum = sparse.csc_matrix((BBlist_val, (BBlist_ix1, BBlist_ix2)), shape=self.XtX.shape, dtype=np.float32)\n",
    "        BBcnt = sparse.csc_matrix((np.ones(len(BBlist_ix1), dtype=np.float32), (BBlist_ix1,BBlist_ix2)), shape=self.XtX.shape, dtype=np.float32)\n",
    "        b_div= sparse.find(BBcnt)[2]\n",
    "        b_3= sparse.find(BBsum)\n",
    "        BBavg = sparse.csc_matrix(( b_3[2] / b_div, (b_3[0],b_3[1])), shape=self.XtX.shape, dtype=np.float32)\n",
    "        BBavg[self.ii_diag]=0.0\n",
    "\n",
    "        # forcing the sparsity pattern of AA onto BB\n",
    "        BBavg = sparse.csr_matrix((np.asarray(BBavg[sparsity_pattern.nonzero()]).flatten(),\n",
    "                                   sparsity_pattern.nonzero()), shape=BBavg.shape, dtype=np.float32)\n",
    "\n",
    "        return BBavg\n",
    "\n",
    "    def train(self):\n",
    "        BBsparse = self.sparse_solution()\n",
    "        #\n",
    "        # rescale items back to original popularity\n",
    "        BBsparse=sparse.diags(scaling).dot(BBsparse).dot(sparse.diags(rescaling))\n",
    "        self.B = BBsparse\n",
    "\n",
    "    def predict(self,predict_matrix):\n",
    "        return predict_matrix.dot(self.B)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## training models + k-fold validation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "K = 10\n",
    "K2 = 20\n",
    "\n",
    "def recal_easer(model, predict_data, test_data):\n",
    "    total = len(test_data)\n",
    "\n",
    "    X_train = data_frame_to_matrix(predict_data)\n",
    "    y_pred = model.predict(X_train).toarray()\n",
    "\n",
    "    X_test = data_frame_to_matrix(test_data)\n",
    "\n",
    "    interacted_recipes = (X_train == 1).toarray()\n",
    "    y_pred[interacted_recipes] = -100000\n",
    "    idx_top_scores = np.asarray((-y_pred).argsort()[:,:20])\n",
    "    del y_pred\n",
    "    dense_X_test = X_test.toarray()\n",
    "\n",
    "    correct_K = 0\n",
    "    correct_K2 = 0\n",
    "    ndcg = 0\n",
    "\n",
    "    for idx, row in enumerate(idx_top_scores):\n",
    "        for rank, index in enumerate(row):\n",
    "            if dense_X_test[idx][index] == 1:\n",
    "                if rank < K:\n",
    "                    correct_K += 1\n",
    "                if rank < K2:\n",
    "                    correct_K2 += 1\n",
    "                if rank < K:\n",
    "                    ndcg += 1/(math.log2(rank+2))\n",
    "\n",
    "    print(\"easer recall@%s = %s\" % (str(K), str(correct_K / total)))\n",
    "    print(\"easer recall@%s = %s\" % (str(K2), str(correct_K2 / total)))\n",
    "    print(\"easer ndcg@%s = %s\" % (10, str(ndcg / total)), end=\"\\n\\n\")\n",
    "\n",
    "    return correct_K/total, correct_K2/total, ndcg/total\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 83,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## training models + k-fold validation\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "#Please enter the path here of where you will place the pickle files (with trailing /)\n",
    "data_path=\"D:/results_aiproject_improvement/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-81-30f462b8f82b>:34: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  scaling = 1.0  / rescaling\n",
      "<ipython-input-81-30f462b8f82b>:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  scaling = 1.0  / rescaling\n",
      "<ipython-input-81-30f462b8f82b>:14: RuntimeWarning: invalid value encountered in multiply\n",
      "  XtX = scaling[:,None] * XtX * scaling\n",
      "C:\\Users\\thiba\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:124: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "C:\\Users\\thiba\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:124: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done training parameter:  100\n",
      "easer recall@20 = 0.014399509803921568\n",
      "easer recall@50 = 0.021752450980392156\n",
      "easer ndcg@100 = 0.008622092073550386\n",
      "\n",
      "fold took :  871.6244850158691 s\n"
     ]
    }
   ],
   "source": [
    "for f_idx, fold_files in enumerate(folds):\n",
    "    start = time.time()\n",
    "    train_data = open_csv(fold_files[0], True)\n",
    "    #Here we have the user item matrix\n",
    "    X_train = data_frame_to_matrix(train_data)\n",
    "\n",
    "    #train models\n",
    "\n",
    "\n",
    "    highest_recall = (0,0,0)\n",
    "    best_lambda = 0\n",
    "    validate_data = open_csv(fold_files[1], use_less_data)\n",
    "    newstart=start\n",
    "    for l in range(100, 1500, 100):\n",
    "        xtx,XtXdiag,ii_diag,rescaling,scaling=precompute_sparse(X_train)\n",
    "        model=sparseMRF(xtx,XtXdiag,ii_diag,rescaling,scaling)\n",
    "        model.train()\n",
    "        print(\"done training parameter: \", l)\n",
    "        interactions, ground_truth = split_test(validate_data)\n",
    "\n",
    "        modelfile = open(data_path+\"model_fold\" + str(f_idx) + \".pkl\", mode='wb')\n",
    "\n",
    "        recall20, recall50, ndcg = recal_easer(model, interactions, ground_truth)\n",
    "        if recall20 > highest_recall[0]:\n",
    "            highest_recall = (recall20, recall50, ndcg)\n",
    "            best_lambda = l\n",
    "\n",
    "            modelfile = open(data_path+\"model_fold\" + str(f_idx) + \".pkl\", mode='wb')\n",
    "            pickle.dump(model, modelfile)\n",
    "            modelfile.close()\n",
    "\n",
    "        end = time.time()\n",
    "        print(\"fold took : \", end - newstart, \"s\")\n",
    "        newstart=end\n",
    "\n",
    "    print(\"done fold:\",str(f_idx))\n",
    "\n",
    "    print(\"Best parameter lambda: \", best_lambda)\n",
    "    print(\"easer fold: %s, recall@%s = %s\" % (str(f_idx), str(K), highest_recall[0]))\n",
    "    print(\"easer fold: %s, recall@%s = %s\" % (str(f_idx), str(K2), highest_recall[1]))\n",
    "    print(\"easer fold: %s, ndcg@%s = %s\" % (str(f_idx), 10, highest_recall[2]), end=\"\\n\\n\")\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"training took : \", end - start, \"s\")\n",
    "    break\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation results of the folds\n",
    "\n",
    "Here we use recall@20, recal@50 and ndcg@100\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "result_list_K = list()\n",
    "result_list_K2 = list()\n",
    "result_ndcg = list()\n",
    "\n",
    "for i in range(k):\n",
    "\n",
    "    #Evaluate recall@k\n",
    "\n",
    "    data = open_csv(folds[i][2])\n",
    "    model = pickle.load(open(data_path+\"model_fold\"+str(i)+\".pkl\", mode='rb'))\n",
    "    interactions, ground_truth = split_test(data)\n",
    "\n",
    "    recall20, recall50, ndcg = recal_easer(model, interactions, ground_truth)\n",
    "\n",
    "    result_list_K.append(recall20)\n",
    "    result_list_K2.append(recall50)\n",
    "    result_ndcg.append(ndcg)\n",
    "\n",
    "    print(\"easer fold: %s, recall@%s = %s\" % (str(i), str(K), recall20))\n",
    "    print(\"easer fold: %s, recall@%s = %s\" % (str(i), str(K2), recall50))\n",
    "    print(\"easer fold: %s, ndcg@%s = %s\" % (str(i), 10, ndcg), end=\"\\n\\n\")\n",
    "\n",
    "print(\"mean recall@%s over 10 folds: \" % str(K), str(st.mean(result_list_K)))\n",
    "print(\"mean recall@%s over 10 folds: \" % str(K2), str(st.mean(result_list_K2)))\n",
    "print(\"mean ndcg@%s over 10 folds: \" % str(100), str(st.mean(result_ndcg)), end=\"\\n\\n\")\n",
    "print(\"standard deviation recall@%s over 10 folds: \" % str(K), str(st.pstdev(result_list_K)))\n",
    "print(\"standard deviation recall@%s over 10 folds: \" % str(K2), str(st.pstdev(result_list_K2)))\n",
    "print(\"standard deviation ndcg@%s over 10 folds: \" % str(100), str(st.pstdev(result_ndcg)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}