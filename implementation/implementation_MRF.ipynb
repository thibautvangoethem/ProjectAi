{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# project ai: Easer\n",
    "\n",
    "by Michiel Téblick and thibaut Van Goethem\n",
    "\n",
    "In this notebook we will look at the easer model proposed at https://dl.acm.org/doi/pdf/10.1145/3308558.3313710.\n",
    "\n",
    "This model will be applied to a dataset from foods.com which containes a bunch of recipes with user ratings/reactions on them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import bottleneck as bn\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import statistics as st\n",
    "\n",
    "## Reading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of interactions in the full dataset:  733951\n",
      "amount of recipes in the full dataset:  80511\n",
      "amount of users in the full dataset:  32635\n",
      "amount of recipes in the smaller dataset:  16577\n",
      "amount of users in the smaller dataset:  31887\n"
     ]
    },
    {
     "data": {
      "text/plain": "         index  user_id  recipe_id        date  rating  \\\n0            0    56680      79222  2006-11-11       5   \n1            1   827374      79222  2010-11-29       3   \n2           10    61995     195977  2008-10-28       5   \n3           20   305531     426090  2017-08-20       5   \n4           21    89831      33096  2004-03-15       5   \n...        ...      ...        ...         ...     ...   \n434723  151900  1423741      39902  2012-07-25       4   \n434724  151907   422893     196735  2009-01-02       5   \n434725  151908    96177     196735  2009-01-12       5   \n434726  151909   573325     196735  2010-09-08       5   \n434727  151911    41468      82303  2006-09-01       5   \n\n                                                   review  count_user  \\\n0       Oh, This was wonderful!  Had a soup and salad ...         174   \n1       We made this last night and really enjoyed it....          10   \n2       I have been making this for more than 10 years...          41   \n3       We really enjoyed these beans. Simple but good...        2195   \n4       Merlot...this is the second time that I made y...        2572   \n...                                                   ...         ...   \n434723  I switched out the American Cheese for Sharp C...           7   \n434724  Yum, Yum, I love potatoes with Rosemary & this...        1130   \n434725  We just loved these tatters. Quick easy and ve...         563   \n434726  What a great, healthy, easy and yummy recipe!<...         880   \n434727  WOW this was great. What I love the most is th...          30   \n\n        count_item  \n0               18  \n1               18  \n2               10  \n3               14  \n4               27  \n...            ...  \n434723          15  \n434724          17  \n434725          17  \n434726          17  \n434727          13  \n\n[434728 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>user_id</th>\n      <th>recipe_id</th>\n      <th>date</th>\n      <th>rating</th>\n      <th>review</th>\n      <th>count_user</th>\n      <th>count_item</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>56680</td>\n      <td>79222</td>\n      <td>2006-11-11</td>\n      <td>5</td>\n      <td>Oh, This was wonderful!  Had a soup and salad ...</td>\n      <td>174</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>827374</td>\n      <td>79222</td>\n      <td>2010-11-29</td>\n      <td>3</td>\n      <td>We made this last night and really enjoyed it....</td>\n      <td>10</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10</td>\n      <td>61995</td>\n      <td>195977</td>\n      <td>2008-10-28</td>\n      <td>5</td>\n      <td>I have been making this for more than 10 years...</td>\n      <td>41</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20</td>\n      <td>305531</td>\n      <td>426090</td>\n      <td>2017-08-20</td>\n      <td>5</td>\n      <td>We really enjoyed these beans. Simple but good...</td>\n      <td>2195</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>21</td>\n      <td>89831</td>\n      <td>33096</td>\n      <td>2004-03-15</td>\n      <td>5</td>\n      <td>Merlot...this is the second time that I made y...</td>\n      <td>2572</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>434723</th>\n      <td>151900</td>\n      <td>1423741</td>\n      <td>39902</td>\n      <td>2012-07-25</td>\n      <td>4</td>\n      <td>I switched out the American Cheese for Sharp C...</td>\n      <td>7</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>434724</th>\n      <td>151907</td>\n      <td>422893</td>\n      <td>196735</td>\n      <td>2009-01-02</td>\n      <td>5</td>\n      <td>Yum, Yum, I love potatoes with Rosemary &amp; this...</td>\n      <td>1130</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>434725</th>\n      <td>151908</td>\n      <td>96177</td>\n      <td>196735</td>\n      <td>2009-01-12</td>\n      <td>5</td>\n      <td>We just loved these tatters. Quick easy and ve...</td>\n      <td>563</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>434726</th>\n      <td>151909</td>\n      <td>573325</td>\n      <td>196735</td>\n      <td>2010-09-08</td>\n      <td>5</td>\n      <td>What a great, healthy, easy and yummy recipe!&lt;...</td>\n      <td>880</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>434727</th>\n      <td>151911</td>\n      <td>41468</td>\n      <td>82303</td>\n      <td>2006-09-01</td>\n      <td>5</td>\n      <td>WOW this was great. What I love the most is th...</td>\n      <td>30</td>\n      <td>13</td>\n    </tr>\n  </tbody>\n</table>\n<p>434728 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_less_data = True\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('../folds/fold_0/train.csv')\n",
    "df_test = pd.read_csv('../folds/fold_0/test.csv')\n",
    "df_validate = pd.read_csv('../folds/fold_0/validate.csv')\n",
    "df = pd.concat([df_train, df_test, df_validate])\n",
    "#df.reset_index()\n",
    "\n",
    "print(\"amount of interactions in the full dataset: \",len(df))\n",
    "print(\"amount of recipes in the full dataset: \",len(df.recipe_id.unique()))\n",
    "print(\"amount of users in the full dataset: \",len(df.user_id.unique()))\n",
    "\n",
    "if use_less_data:\n",
    "    df = df[df['count_item'] >= 10]\n",
    "    print(\"amount of recipes in the smaller dataset: \",len(df.recipe_id.unique()))\n",
    "    print(\"amount of users in the smaller dataset: \",len(df.user_id.unique()))\n",
    "df.reset_index()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set all ratings to 1 (even negative interactions are seen as interactions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.loc[:,'rating'] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below here are two ways to cut down on the amount of interactions that are used in this notebook\n",
    "- The first one randomly removes x% of the users,\n",
    "- The second one removes all user and recipes that have less than X amount of interaction containing them\n",
    "\n",
    "We opted for the second form as this is more representative of how the models should be used due to the lower amount if recipes but more reactions per recipe. Also the second choice is a deterministic way of removing data, which the first one is not.\n",
    "This does end up mostly giving slightly worse result compared to the first choice.\n",
    "\n",
    "The reason we need to remove data is because a matrix inversion is done, which can not be done in a smart way.\n",
    "Also the result of the inversion is not necessarily a sparse matrix so the full calculation needs to be done on dense matrices. This end up scaling O(n^3) in time complexity and O(n^2) for memory needed. n here is the amount of recipes.\n",
    "So running on the full dataset would require more than 200gb of ram which we do not have.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### rescaling the id's\n",
    "The recipes and users don't go from 0 to amount so if we were to put this in a matrix we would get empty columns and rows. This is not that handy so we reindex both the user_id and recipe_ids\n",
    "\n",
    "This is a step we must not forget when entering the data in the model, as we also need to remap our input data using the same remapping that was used here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "userSet = set(df['user_id'].to_list())\n",
    "user_transform_dict = dict(map(reversed, enumerate(userSet)))\n",
    "recipeSet = set(df['recipe_id'].to_list())\n",
    "recipe_transform_dict = dict(map(reversed, enumerate(recipeSet)))\n",
    "recipe_dict = dict(enumerate(recipeSet))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "keep_nan_user = [k for k, v in user_transform_dict.items() if pd.isnull(v)]\n",
    "keep_nan_recipe = [k for k, v in recipe_transform_dict.items() if pd.isnull(v)]\n",
    "\n",
    "\n",
    "def transform_id(dataframe):\n",
    "    tochange = dataframe['user_id']\n",
    "    dataframe['user_id'] = tochange.map(user_transform_dict).fillna(tochange.mask(tochange.isin(keep_nan_user)))\n",
    "\n",
    "    tochange = dataframe['recipe_id']\n",
    "    dataframe['recipe_id'] = tochange.map(recipe_transform_dict).fillna(tochange.mask(tochange.isin(keep_nan_recipe)))\n",
    "    return dataframe\n",
    "\n",
    "def open_csv(filename, use_less_data=False):\n",
    "    df = pd.read_csv(filename)\n",
    "    if use_less_data:\n",
    "        df = df[df['count_item'] >= 10]\n",
    "    df = transform_id(df)\n",
    "    df.loc[:,'rating'] = 1\n",
    "    df.drop('review', axis=1, inplace=True)\n",
    "    #df.drop('date', axis=1, inplace=True)\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creation of the folds\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "k = 10\n",
    "folds = list()\n",
    "for directory in [\"../folds/fold_%d\" % i for i in range(k)]:\n",
    "    folds.append((directory + \"/test.csv\", directory + \"/train.csv\", directory + \"/validate.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creation model\n",
    "Here we define the models used for the experiments. Both the easer predictor and a populaliry predictor are created. the popularity predictor is used as a baseline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def split_test(data_set):\n",
    "    ground_truth = data_set.sort_values('date').groupby('user_id').tail(1)\n",
    "    predict = pd.concat([data_set, ground_truth]).drop_duplicates(keep=False)\n",
    "    return predict, ground_truth\n",
    "\n",
    "def data_frame_to_matrix(dataframe):\n",
    "    ratings = dataframe.rating\n",
    "    idx = (dataframe.user_id, dataframe.recipe_id)\n",
    "    return sparse.csc_matrix((ratings, idx), shape=(len(df.user_id.unique()), len(df.recipe_id.unique())),\n",
    "                                dtype=float)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creation model\n",
    "Here we define the models used for the experiments. Both the easer predictor and a populaliry predictor are created. the popularity predictor is used as a baseline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "alpha = 0.75\n",
    "def precompute(train_data):\n",
    "    userCount = train_data.shape[0]\n",
    "    XtX= np.asarray(train_data.T.dot(train_data).todense(), dtype = np.float32)\n",
    "    #del train_data  # only the item-item data-matrix XtX is needed in the following\n",
    "\n",
    "    mu=np.diag(XtX) / userCount   # the mean of the columns in train_data (for binary train_data)\n",
    "    variance_times_userCount = np.diag(XtX) - mu * mu * userCount # variances of columns in train_data (scaled by userCount)\n",
    "\n",
    "    # standardizing the data-matrix XtX (if alpha=1, then XtX becomes the correlation matrix)\n",
    "    XtX -= mu[:,None] *(mu* userCount)\n",
    "    rescaling = np.power(variance_times_userCount, alpha / 2.0)\n",
    "    scaling = 1.0  / rescaling\n",
    "    XtX = scaling[:,None] * XtX * scaling\n",
    "\n",
    "    XtXdiag = deepcopy(np.diag(XtX))\n",
    "    ii_diag = np.diag_indices(XtX.shape[0])\n",
    "\n",
    "    # print(\"number of items: {}\".format(len(mu)))\n",
    "    # print(\"number of users: {}\".format(userCount))\n",
    "    return XtX,XtXdiag,ii_diag,rescaling,scaling\n",
    "\n",
    "def precompute_sparse(train_data):\n",
    "    userCount = train_data.shape[0]\n",
    "    XtX= train_data.T.dot(train_data)\n",
    "    #del train_data  # only the item-item data-matrix XtX is needed in the following\n",
    "\n",
    "    mu=XtX.diagonal() / userCount\n",
    "    variance_times_userCount = XtX.diagonal() - mu * mu * userCount\n",
    "\n",
    "    # standardizing the data-matrix XtX (if alpha=1, then XtX becomes the correlation matrix)\n",
    "    XtX -= scipy.sparse.csr_matrix(mu[:,None]) * scipy.sparse.csr_matrix(mu* userCount)\n",
    "    rescaling = np.power(variance_times_userCount, alpha / 2.0)\n",
    "    scaling = 1.0  / rescaling\n",
    "\n",
    "    XtX = scipy.sparse.diags(scaling) * XtX * scipy.sparse.diags(scaling)\n",
    "\n",
    "    XtXdiag = deepcopy(XtX.diagonal())\n",
    "    ii_diag = np.diag_indices(XtX.shape[0])\n",
    "\n",
    "    # print(\"number of items: {}\".format(len(mu)))\n",
    "    # print(\"number of users: {}\".format(userCount))\n",
    "    return XtX,XtXdiag,ii_diag,rescaling,scaling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class sparseMRF:\n",
    "    def __init__(self,xtx,xtxdiag,ii,rescaling,scaling):\n",
    "        self.XtX=xtx\n",
    "        self.XtXdiag = xtxdiag\n",
    "        self.ii_diag = ii\n",
    "        self.threshold = 0.375    # results in sparsity 0.1 % (for alpha=0.75)\n",
    "        #threshold = 0.11    # results in sparsity 0.5 % (for alpha=0.75)\n",
    "        self.maxInColumn = 1000\n",
    "        # hyper-parameter r in the paper, which determines the trade-off between approximation-accuracy and training-time\n",
    "        self.rr = 0.5\n",
    "        # L2 norm regularization\n",
    "        self.L2reg = 1.0\n",
    "        self.scaling=scaling\n",
    "        self.rescaling=rescaling\n",
    "    def sparse_solution(self):\n",
    "\n",
    "        # sparsity pattern, see section 3.1 in the paper\n",
    "        self.XtX[self.ii_diag] = XtXdiag\n",
    "        AA = self.calculate_sparsity_pattern()\n",
    "\n",
    "        # parameter-estimation, see section 3.2 in the paper\n",
    "        self.XtX[self.ii_diag] = self.XtXdiag+self.L2reg\n",
    "        BBsparse = self.sparse_parameter_estimation(AA)\n",
    "\n",
    "        return BBsparse\n",
    "\n",
    "\n",
    "    def calculate_sparsity_pattern(self):\n",
    "        # this implements section 3.1 in the paper.\n",
    "\n",
    "        # print(\"sparsifying the data-matrix (section 3.1 in the paper) ...\")\n",
    "        # apply threshold\n",
    "        # ix = np.where( np.abs(self.XtX) > self.threshold)\n",
    "        # AA= sparse.csc_matrix( (self.XtX[ix], ix), shape=self.XtX.shape, dtype=np.float32)\n",
    "        idx1, idx2, value = sparse.find(np.abs(self.XtX > self.threshold))\n",
    "        boolean_matrix = scipy.sparse.csc_matrix((value, (idx1, idx2)), dtype=np.float32)\n",
    "        AA = boolean_matrix.multiply(self.XtX)\n",
    "\n",
    "        # enforce maxInColumn, see section 3.1 in paper\n",
    "        countInColumns=AA.getnnz(axis=0)\n",
    "        iiList = np.where(countInColumns > self.maxInColumn)[0]\n",
    "        # print(\"    number of items with more than {} entries in column: {}\".format(self.maxInColumn, len(iiList)) )\n",
    "        for ii in iiList:\n",
    "            jj= AA[:,ii].nonzero()[0]\n",
    "            kk = bn.argpartition(-np.abs(np.asarray(AA[jj,ii].todense()).flatten()), self.maxInColumn)[self.maxInColumn:]\n",
    "            AA[  jj[kk], ii ] = 0.0\n",
    "        AA.eliminate_zeros()\n",
    "        # print(\"    resulting sparsity of AA: {}\".format( AA.nnz*1.0 / AA.shape[0] / AA.shape[0]) )\n",
    "\n",
    "        return AA\n",
    "\n",
    "    def sparse_parameter_estimation(self, AA):\n",
    "        # this implements section 3.2 in the paper\n",
    "\n",
    "        # list L in the paper, sorted by item-counts per column, ties broken by item-popularities as reflected by np.diag(XtX)\n",
    "        AAcountInColumns = AA.getnnz(axis=0)\n",
    "        sortedList=np.argsort(AAcountInColumns+ self.XtX.diagonal() /2.0/ np.max(self.XtX.diagonal()))[::-1]\n",
    "\n",
    "        # print(\"iterating through steps 1,2, and 4 in section 3.2 of the paper ...\")\n",
    "        todoIndicators=np.ones(AAcountInColumns.shape[0])\n",
    "        blockList=[]   # list of blocks. Each block is a list of item-indices, to be processed in step 3 of the paper\n",
    "        for ii in sortedList:\n",
    "            if todoIndicators[ii]==1:\n",
    "                nn, _, vals=sparse.find(AA[:,ii])  # step 1 in paper: set nn contains item ii and its neighbors N\n",
    "                kk=np.argsort(np.abs(vals))[::-1]\n",
    "                nn=nn[kk]\n",
    "                blockList.append(nn) # list of items in the block, to be processed in step 3 below\n",
    "                # remove possibly several items from list L, as determined by parameter rr (r in the paper)\n",
    "                dd_count=max(1,int(np.ceil(len(nn)*self.rr)))\n",
    "                dd=nn[:dd_count] # set D, see step 2 in the paper\n",
    "                todoIndicators[dd]=0  # step 4 in the paper\n",
    "\n",
    "        # print(\"now step 3 in section 3.2 of the paper: iterating ...\")\n",
    "        # now the (possibly heavy) computations of step 3:\n",
    "        # given that steps 1,2,4 are already done, the following for-loop could be implemented in parallel.\n",
    "        BBlist_ix1, BBlist_ix2, BBlist_val = [], [], []\n",
    "        denseXtX = self.XtX.toarray()\n",
    "        for nn in blockList:\n",
    "            #calculate dense solution for the items in set nn\n",
    "            BBblock=np.linalg.inv(denseXtX[np.ix_(nn,nn)])\n",
    "            BBblock/=-np.diag(BBblock)\n",
    "            # determine set D based on parameter rr (r in the paper)\n",
    "            dd_count=max(1,int(np.ceil(len(nn)*self.rr)))\n",
    "            dd=nn[:dd_count] # set D in paper\n",
    "            # store the solution regarding the items in D\n",
    "            blockix = np.meshgrid(dd,nn)\n",
    "            BBlist_ix1.extend(blockix[1].flatten().tolist())\n",
    "            BBlist_ix2.extend(blockix[0].flatten().tolist())\n",
    "            BBlist_val.extend(BBblock[:,:dd_count].flatten().tolist())\n",
    "\n",
    "        del denseXtX\n",
    "\n",
    "        # print(\"final step: obtaining the sparse matrix BB by averaging the solutions regarding the various sets D ...\")\n",
    "        BBsum = sparse.csc_matrix( (BBlist_val,  (BBlist_ix1, BBlist_ix2  )  ), shape=self.XtX.shape, dtype=np.float32)\n",
    "        BBcnt = sparse.csc_matrix( (np.ones(len(BBlist_ix1), dtype=np.float32),  (BBlist_ix1,BBlist_ix2  )  ), shape=self.XtX.shape, dtype=np.float32)\n",
    "        b_div= sparse.find(BBcnt)[2]\n",
    "        b_3= sparse.find(BBsum)\n",
    "        BBavg = sparse.csc_matrix( ( b_3[2] / b_div   ,  (b_3[0],b_3[1]  )  ), shape=self.XtX.shape, dtype=np.float32)\n",
    "        BBavg[self.ii_diag]=0.0\n",
    "\n",
    "        # print(\"forcing the sparsity pattern of AA onto BB ...\")\n",
    "        BBavg = sparse.csr_matrix( ( np.asarray(BBavg[AA.nonzero()]).flatten(),  AA.nonzero() ), shape=BBavg.shape, dtype=np.float32)\n",
    "\n",
    "        # print(\"    resulting sparsity of learned BB: {}\".format( BBavg.nnz * 1.0 / AA.shape[0] / AA.shape[0]) )\n",
    "\n",
    "        return BBavg\n",
    "\n",
    "    def train(self):\n",
    "        # print(\"training the sparse model:\\n\")\n",
    "        BBsparse = self.sparse_solution()\n",
    "        # print(\"\\ntotal training time (including the time for determining the sparsity-pattern):\")\n",
    "        #\n",
    "        # print(\"\\nre-scaling BB back to the original item-popularities ...\")\n",
    "        # assuming that mu.T.dot(BB) == mu, see Appendix in paper\n",
    "        BBsparse=sparse.diags(scaling).dot(BBsparse).dot(sparse.diags(rescaling))\n",
    "\n",
    "        # print(\"\\nfor the evaluation below: converting the sparse model into a dense-matrix-representation ...\")\n",
    "        self.BB = BBsparse\n",
    "    def predict(self,predict_matrix):\n",
    "        return predict_matrix.dot(self.BB)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## training models + k-fold validation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "K = 20\n",
    "K2 = 50\n",
    "\n",
    "def recal_easer(model, predict_data, test_data):\n",
    "    total = len(test_data)\n",
    "\n",
    "    X_train = data_frame_to_matrix(predict_data)\n",
    "    y_pred = model.predict(X_train).toarray()\n",
    "\n",
    "    X_test = data_frame_to_matrix(test_data)\n",
    "\n",
    "    interacted_recipes = (X_train == 1).toarray()\n",
    "    y_pred[interacted_recipes] = -100000\n",
    "    idx_top_scores = np.asarray((-y_pred).argsort()[:,:100])\n",
    "    del y_pred\n",
    "    dense_X_test = X_test.toarray()\n",
    "\n",
    "    correct_K = 0\n",
    "    correct_K2 = 0\n",
    "    ndcg = 0\n",
    "\n",
    "    for idx, row in enumerate(idx_top_scores):\n",
    "        for rank, index in enumerate(row):\n",
    "            if dense_X_test[idx][index] == 1:\n",
    "                if rank < K:\n",
    "                    correct_K += 1\n",
    "                if rank < K2:\n",
    "                    correct_K2 += 1\n",
    "                ndcg += 1/(math.log2(rank+2))\n",
    "\n",
    "    print(\"easer recall@%s = %s\" % (str(K), str(correct_K / total)))\n",
    "    print(\"easer recall@%s = %s\" % (str(K2), str(correct_K2 / total)))\n",
    "    print(\"easer ndcg@%s = %s\" % (100, str(ndcg / total)), end=\"\\n\\n\")\n",
    "\n",
    "    return correct_K/total, correct_K2/total, ndcg/total\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## training models + k-fold validation\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-30f462b8f82b>:34: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  scaling = 1.0  / rescaling\n",
      "<ipython-input-8-30f462b8f82b>:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  scaling = 1.0  / rescaling\n",
      "<ipython-input-8-30f462b8f82b>:14: RuntimeWarning: invalid value encountered in multiply\n",
      "  XtX = scaling[:,None] * XtX * scaling\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done training parameter:  100\n",
      "saved\n",
      "easer recall@20 = 0.010704048728054461\n",
      "easer recall@50 = 0.016391974202794698\n",
      "easer ndcg@100 = 0.007988050276139585\n",
      "\n",
      "fold took :  264.875864982605 s\n",
      "done fold: 0\n",
      "Best parameter lambda:  100\n",
      "easer fold: 0, recall@20 = 0.010704048728054461\n",
      "easer fold: 0, recall@50 = 0.016391974202794698\n",
      "easer fold: 0, ndcg@100 = 0.007988050276139585\n",
      "\n",
      "training took :  264.88905692100525 s\n"
     ]
    }
   ],
   "source": [
    "#Please enter the path here of where you will place the pickle files (with trailing /)\n",
    "data_path=\"../results_aiproject/\"\n",
    "for f_idx, fold_files in enumerate(folds):\n",
    "    start = time.time()\n",
    "    train_data = open_csv(fold_files[0], True)\n",
    "    #Here we have the user item matrix\n",
    "    X_train = data_frame_to_matrix(train_data)\n",
    "\n",
    "    #train models\n",
    "\n",
    "    # model_pop=popularity()\n",
    "    # model_pop.train(train_data)\n",
    "    # modelpopfile = open(data_path+\"model_pop_fold\" + str(f_idx) + \".pkl\", mode='wb')\n",
    "    # pickle.dump(model_pop, modelpopfile)\n",
    "    # modelpopfile.close()\n",
    "    # model_pop = None\n",
    "\n",
    "    highest_recall = (0,0,0)\n",
    "    best_lambda = 0\n",
    "    validate_data = open_csv(fold_files[1], use_less_data)\n",
    "    newstart=start\n",
    "    for l in range(100, 1500, 100):\n",
    "        xtx,XtXdiag,ii_diag,rescaling,scaling=precompute_sparse(X_train)\n",
    "        xtx2,XtXdiag2,ii_diag2,rescaling2,scaling2=precompute(X_train)\n",
    "        model=sparseMRF(xtx,XtXdiag,ii_diag,rescaling,scaling)\n",
    "        model.train()\n",
    "        print(\"done training parameter: \", l)\n",
    "        interactions, ground_truth = split_test(validate_data)\n",
    "\n",
    "        modelfile = open(data_path+\"model_fold\" + str(f_idx) + \".pkl\", mode='wb')\n",
    "        pickle.dump(model, modelfile)\n",
    "        modelfile.close()\n",
    "\n",
    "        recall20, recall50, ndcg = recal_easer(model, interactions, ground_truth)\n",
    "        if recall20 > highest_recall[0]:\n",
    "            highest_recall = (recall20, recall50, ndcg)\n",
    "            best_lambda = l\n",
    "\n",
    "            modelfile = open(data_path+\"model_fold\" + str(f_idx) + \".pkl\", mode='wb')\n",
    "            pickle.dump(model, modelfile)\n",
    "            modelfile.close()\n",
    "\n",
    "        end = time.time()\n",
    "        print(\"fold took : \", end - newstart, \"s\")\n",
    "        newstart=end\n",
    "\n",
    "    print(\"done fold:\",str(f_idx))\n",
    "\n",
    "    print(\"Best parameter lambda: \", best_lambda)\n",
    "    print(\"easer fold: %s, recall@%s = %s\" % (str(f_idx), str(K), highest_recall[0]))\n",
    "    print(\"easer fold: %s, recall@%s = %s\" % (str(f_idx), str(K2), highest_recall[1]))\n",
    "    print(\"easer fold: %s, ndcg@%s = %s\" % (str(f_idx), 100, highest_recall[2]), end=\"\\n\\n\")\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"training took : \", end - start, \"s\")\n",
    "    break\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation results of the folds\n",
    "\n",
    "Here we use recall@20, recal@50 and ndcg@100\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "result_list_K = list()\n",
    "result_list_K2 = list()\n",
    "result_ndcg = list()\n",
    "\n",
    "for i in range(k):\n",
    "\n",
    "    #Evaluate recall@k\n",
    "\n",
    "    data = open_csv(folds[k][2])\n",
    "    model = pickle.load(open(data_path+\"model_fold\"+str(i)+\".pkl\", mode='rb'))\n",
    "    interactions, ground_truth = split_test(data)\n",
    "\n",
    "    recall20, recall50, ndcg = recal_easer(model, interactions, ground_truth)\n",
    "\n",
    "    result_list_K.append(recall20)\n",
    "    result_list_K2.append(recall50)\n",
    "    result_ndcg.append(ndcg)\n",
    "\n",
    "    print(\"easer fold: %s, recall@%s = %s\" % (str(i), str(K), recall20))\n",
    "    print(\"easer fold: %s, recall@%s = %s\" % (str(i), str(K2), recall50))\n",
    "    print(\"easer fold: %s, ndcg@%s = %s\" % (str(i), 100, ndcg), end=\"\\n\\n\")\n",
    "\n",
    "print(\"mean recall@%s over 10 folds: \" % str(K), str(st.mean(result_list_K)))\n",
    "print(\"mean recall@%s over 10 folds: \" % str(K2), str(st.mean(result_list_K2)))\n",
    "print(\"mean ndcg@%s over 10 folds: \" % str(100), str(st.mean(result_ndcg)), end=\"\\n\\n\")\n",
    "print(\"standard deviation recall@%s over 10 folds: \" % str(K), str(st.pstdev(result_list_K)))\n",
    "print(\"standard deviation recall@%s over 10 folds: \" % str(K2), str(st.pstdev(result_list_K2)))\n",
    "print(\"standard deviation ndcg@%s over 10 folds: \" % str(100), str(st.pstdev(result_ndcg)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#recall score for popularity\n",
    "result_list_pop_K=list()\n",
    "result_list_pop_K2=list()\n",
    "result_list_pop_ndcg=list()\n",
    "for i in range(k):\n",
    "    data = pickle.load(open(data_path+\"data_fold\"+str(i)+\".pkl\", mode='rb'))\n",
    "    model = pickle.load(open(data_path+\"model_pop_fold\"+str(i)+\".pkl\", mode='rb'))\n",
    "    test_data = data[1]\n",
    "    predict_data = data[0]\n",
    "    pop=model.predict()\n",
    "    total = 0\n",
    "    correct_K = 0\n",
    "    correct_K2 = 0\n",
    "    ndcg = 0\n",
    "    for idx, interaction in test_data.iterrows():\n",
    "        user = interaction['user_id']\n",
    "        user_data = predict_data.loc[(predict_data['user_id'] == user)]\n",
    "        already_interacted_recipes = user_data[user_data.columns[1]].to_numpy()\n",
    "        newpop = pop[:150]\n",
    "        newpop = newpop[~np.in1d(newpop,already_interacted_recipes)]\n",
    "        newpop_K = newpop[:K]\n",
    "        newpop_K2 = newpop[:K2]\n",
    "        newpop_ndcg = newpop[:100]\n",
    "        recipe = interaction['recipe_id']\n",
    "        if recipe in newpop_K:\n",
    "            correct_K += 1\n",
    "        if recipe in newpop_K2:\n",
    "            correct_K2 += 1\n",
    "        if recipe in newpop_ndcg:\n",
    "            ndcg += 1/(math.log2(np.where(newpop_ndcg == recipe)[0]+2))\n",
    "        total += 1\n",
    "    result_list_pop_K.append(correct_K / total)\n",
    "    result_list_pop_K2.append(correct_K2 / total)\n",
    "    result_list_pop_ndcg.append(ndcg / total)\n",
    "    print(\"popularity fold: %s, recall@%s = %s\" % (str(i),str(K), str(correct_K / total)))\n",
    "    print(\"popularity fold: %s, recall@%s = %s\" % (str(i),str(K2), str(correct_K2 / total)))\n",
    "    print(\"popularity fold: %s, ndcg@%s = %s\" % (str(i),str(100), str(ndcg / total)), end=\"\\n\\n\")\n",
    "\n",
    "print(\"mean recall@%s over 10 folds: \" % str(K), str(st.mean(result_list_pop_K)))\n",
    "print(\"mean recall@%s over 10 folds: \" % str(K2), str(st.mean(result_list_pop_K2)))\n",
    "print(\"mean ndcg@%s over 10 folds: \" % str(100), str(st.mean(result_list_pop_ndcg)), end=\"\\n\\n\")\n",
    "print(\"standard deviation recall@%s over 10 folds: \" % str(K), str(st.pstdev(result_list_pop_K)))\n",
    "print(\"standard deviation recall@%s over 10 folds: \" % str(K2), str(st.pstdev(result_list_pop_K2)))\n",
    "print(\"standard deviation ndcg@%s over 10 folds: \" % str(100), str(st.pstdev(result_list_pop_ndcg)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next section is a demonstration that selects a random user and makes a recommendation prediction for this user."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "# read recipe data and load pre-trained model\n",
    "df_recipes = pd.read_csv('../data/RAW_recipes.csv')\n",
    "df_recipes.drop(['minutes', 'contributor_id', 'submitted', 'tags',\n",
    "                 'nutrition', 'n_steps', 'steps', 'description', 'n_ingredients'], axis=1, inplace=True)\n",
    "data = pickle.load(open(data_path+\"data_fold0.pkl\", mode='rb'))\n",
    "model = pickle.load(open(data_path+\"model_fold0.pkl\", mode='rb'))\n",
    "predict_data = data[0]\n",
    "ratings = predict_data.rating\n",
    "idx = (predict_data.user_id, predict_data.recipe_id)\n",
    "x_train = sparse.csc_matrix((ratings, idx), shape=(len(df.user_id.unique()), len(df.recipe_id.unique())), dtype=float)\n",
    "\n",
    "# get random user and make prediction\n",
    "random_user = x_train.getrow(random.randint(0, len(df.user_id.unique())))\n",
    "prediction = model.predict(random_user)[0]\n",
    "interacted_recipes = []\n",
    "for recipe_id in random_user.indices:\n",
    "    interacted_recipes.append(recipe_dict[recipe_id])\n",
    "    prediction[recipe_id] = -100000\n",
    "\n",
    "\n",
    "top_index = (-prediction).argsort()[:10]\n",
    "recommended_recipes = []\n",
    "for recipe_id in top_index:\n",
    "    recommended_recipes.append(recipe_dict[recipe_id])\n",
    "\n",
    "# get interacted recipes and recommended recipes\n",
    "user_interactions = df_recipes[df_recipes['id'].isin(interacted_recipes)].drop('id', axis=1)\n",
    "user_recommendations = df_recipes[df_recipes['id'].isin(recommended_recipes)].drop('id', axis=1)\n",
    "\n",
    "display(user_interactions)\n",
    "display(user_recommendations)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}