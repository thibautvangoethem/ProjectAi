POPULARITY:

popularity fold: 0, recall@5,10,20 = 0.012867647058823529,0.020833333333333332,0.03630514705882353
popularity fold: 0, ndcg@5,10,20 = 0.007077422367738754,0.007077422367738754,0.007077422367738754

popularity fold: 1, recall@5,10,20 = 0.014399509803921568,0.022365196078431373,0.037837009803921566
popularity fold: 1, ndcg@5,10,20 = 0.008112613931785705,0.008112613931785705,0.008112613931785705

popularity fold: 2, recall@5,10,20 = 0.012867647058823529,0.023131127450980393,0.036917892156862746
popularity fold: 2, ndcg@5,10,20 = 0.007923687283231338,0.007923687283231338,0.007923687283231338

popularity fold: 3, recall@5,10,20 = 0.014248506204994637,0.024513559062356367,0.03615749961697564
popularity fold: 3, ndcg@5,10,20 = 0.009134042495342935,0.009134042495342935,0.009134042495342935

popularity fold: 4, recall@5,10,20 = 0.014863622433343548,0.02359791602819491,0.03738890591480233
popularity fold: 4, ndcg@5,10,20 = 0.00908596350703251,0.00908596350703251,0.00908596350703251

popularity fold: 5, recall@5,10,20 = 0.013024823781795893,0.022678516702421086,0.03830830524057616
popularity fold: 5, ndcg@5,10,20 = 0.008270421576151274,0.008270421576151274,0.008270421576151274

popularity fold: 6, recall@5,10,20 = 0.011645724793135151,0.021605884155684953,0.03738890591480233
popularity fold: 6, ndcg@5,10,20 = 0.007304098444535143,0.007304098444535143,0.007304098444535143

popularity fold: 7, recall@5,10,20 = 0.013024823781795893,0.02221881703953417,0.038461538461538464
popularity fold: 7, ndcg@5,10,20 = 0.007438811132523918,0.007438811132523918,0.007438811132523918

popularity fold: 8, recall@5,10,20 = 0.014401715949134366,0.021908993411981003,0.03554466064041673
popularity fold: 8, ndcg@5,10,20 = 0.008386705223303636,0.008386705223303636,0.008386705223303636

popularity fold: 9, recall@5,10,20 = 0.011795343137254902,0.0196078431372549,0.03170955882352941
popularity fold: 9, ndcg@5,10,20 = 0.006410860815943716,0.006410860815943716,0.006410860815943716

mean recall@5,10,20 over 10 folds: 0.013313936400302302,0.02224611864001725,0.03660194236322489
mean ndcg@5,10,20 over 10 folds: 0.007914462677758893,0.007914462677758893,0.007914462677758893

standard deviation recall@5,10,20 over 10 folds: 0.0010626032896713206,0.0013203707550035772,0.0018593678814191907
standard deviation ndcg@5,10,20 over 10 folds: 0.0008251728198122395,0.0008251728198122395,0.0008251728198122395

RMF:
fold 0
done training parameter:  0
easer recall@5,10,20 = 0.001991421568627451,0.0026041666666666665,0.003216911764705882
easer ndcg@5,10,20 = 0.001733690160126828,0.0019108132154686713,0.002069025435686995

fold took :  986.1543245315552 s
done training parameter:  100
easer recall@5,10,20 = 0.002297794117647059,0.0027573529411764708,0.0035232843137254903
easer ndcg@5,10,20 = 0.001869543837705649,0.002012671417323265,0.0022116859505777403

done training parameter:  200
easer recall@5,10,20 = 0.002297794117647059,0.0027573529411764708,0.0035232843137254903
easer ndcg@5,10,20 = 0.0018762570348316984,0.0020193846144493147,0.0022183991477037904

fold took :  1063.8686299324036 s

done training parameter:  300
easer recall@5,10,20 = 0.002297794117647059,0.0027573529411764708,0.0035232843137254903
easer ndcg@5,10,20 = 0.0018762570348316984,0.0020193846144493147,0.0022183991477037904

fold took :  1096.4854952199598 s

done training parameter:  400
easer recall@5,10,20 = 0.001991421568627451,0.0024509803921568627,0.0029105392156862746
easer ndcg@5,10,20 = 0.001694486260975008,0.0018416579659706633,0.00196499422897438

fold took :  917.8929147720337 s

done training parameter:  500
easer recall@5,10,20 = 0.001991421568627451,0.0024509803921568627,0.0029105392156862746
easer ndcg@5,10,20 = 0.001694486260975008,0.0018416579659706633,0.00196499422897438

fold took :  910.5911266803741 s

Switched to a lower threshold from here after improving efficiency of recall calculation

done training parameter:  0
easer recall@5,10,20 = 0.006587009803921569,0.00980392156862745,0.015625
easer ndcg@5,10,20 = 0.004671635718636355,0.0056981030178982,0.00718346981758031

fold took :  454.1183156967163 s
done training parameter:  100
easer recall@5,10,20 = 0.010876225490196078,0.018075980392156864,0.027573529411764705
easer ndcg@5,10,20 = 0.007690353392325058,0.009986656287833311,0.012371753964050207

fold took :  450.3934187889099 s
done training parameter:  200
easer recall@5,10,20 = 0.010876225490196078,0.017922794117647058,0.02803308823529412
easer ndcg@5,10,20 = 0.007693160386772225,0.009947948097348542,0.012482170034844593

fold took :  459.4225091934204 s
done training parameter:  300
easer recall@5,10,20 = 0.011029411764705883,0.018075980392156864,0.028186274509803922
easer ndcg@5,10,20 = 0.007736270488602929,0.009990153896605482,0.012514771887825073

fold took :  455.6222183704376 s
done fold: 0
Best parameter lambda:  300
easer fold: 0, recall@5 = 0.011029411764705883
easer fold: 0, recall@10 = 0.018075980392156864
easer fold: 0, ndcg@100 = 0.028186274509803922

training took :  1819.5564620494843 s

fold 1
done training parameter:  0
easer recall@5,10,20 = 0.011642156862745098,0.015318627450980392,0.020833333333333332
easer ndcg@5,10,20 = 0.008073129253560227,0.009243416030101282,0.010620590943727975

fold took :  916.1794080734253 s
done training parameter:  100
easer recall@5,10,20 = 0.01332720588235294,0.019301470588235295,0.029871323529411766
easer ndcg@5,10,20 = 0.00908091837327618,0.01097488675695415,0.01364216684086151

fold took :  826.0073885917664 s
done training parameter:  200
easer recall@5,10,20 = 0.012867647058823529,0.020067401960784315,0.029871323529411766
easer ndcg@5,10,20 = 0.00885986075006162,0.011143741992288675,0.013609856289424462

fold took :  911.5332636833191 s
done training parameter:  300
easer recall@5,10,20 = 0.012714460784313725,0.01991421568627451,0.029871323529411766
easer ndcg@5,10,20 = 0.008787256765691651,0.011076788439793707,0.013579058245803608

fold took :  946.0159134864807 s
done fold: 1
Best parameter lambda:  100
easer fold: 1, recall@5 = 0.01332720588235294
easer fold: 1, recall@10 = 0.019301470588235295
easer fold: 1, recall@20 = 0.029871323529411766
easer fold: 1, ndcg@5 = 0.00908091837327618
easer fold: 1, ndcg@10 = 0.01097488675695415
easer fold: 1, ndcg@20 = 0.01364216684086151

FOLD 2

done training parameter:  3
easer recall@5,10,20 = 0.01133578431372549,0.01608455882352941,0.024509803921568627
easer ndcg@5,10,20 = 0.008119046349838097,0.009649215692044714,0.011744271712851287

fold took :  955.9702363014221 s
done training parameter:  6
easer recall@5,10,20 = 0.01133578431372549,0.016697303921568627,0.02542892156862745
easer ndcg@5,10,20 = 0.008139375766255588,0.009878693406805911,0.012065114574185016

fold took :  884.2432522773743 s
done training parameter:  9
easer recall@5,10,20 = 0.011795343137254902,0.016697303921568627,0.025888480392156864
easer ndcg@5,10,20 = 0.008227220805926392,0.009802977835056742,0.01213260367368344

fold took :  910.7731187343597 s
done training parameter:  12
easer recall@5,10,20 = 0.012254901960784314,0.01761642156862745,0.026348039215686275
easer ndcg@5,10,20 = 0.008405002426898149,0.010088608719988349,0.012285104791879019

fold took :  959.8873355388641 s
done training parameter:  15
easer recall@5,10,20 = 0.012254901960784314,0.017922794117647058,0.027113970588235295
easer ndcg@5,10,20 = 0.00837941474703777,0.010146648328153969,0.012439838948233237

fold took :  913.6519565582275 s

change to lower rr and maxInColumn values
FOLD 3

done training parameter:  0
easer recall@5,10,20 = 0.010418262601501456,0.015320974413972729,0.02206220315612073
easer ndcg@5,10,20 = 0.007566278164515099,0.009106776374211877,0.010805528911111173

fold took :  911.577791929245 s
done training parameter:  100
easer recall@5,10,20 = 0.013329247740156274,0.020223686226444004,0.030182319595526277
easer ndcg@5,10,20 = 0.009377281560331786,0.011569938963567885,0.01411533513973763

fold took :  895.7984080314636 s
done training parameter:  200
easer recall@5,10,20 = 0.013329247740156274,0.020530105714723457,0.030335529339666004
easer ndcg@5,10,20 = 0.009343603540857911,0.011622084228117429,0.014107368659079366

fold took :  941.6258816719055 s
done training parameter:  300
easer recall@5,10,20 = 0.013176037996016547,0.02037689597058373,0.030795158572085184
easer ndcg@5,10,20 = 0.009238409790017329,0.01153266677498677,0.01417018817482774

fold took :  853.9622120857239 s
done fold: 3
Best parameter lambda:  300
easer fold: 3, recall@5 = 0.013176037996016547
easer fold: 3, recall@10 = 0.02037689597058373
easer fold: 3, recall@20 = 0.030795158572085184
easer fold: 3, ndcg@5 = 0.009238409790017329
easer fold: 3, ndcg@10 = 0.01153266677498677
easer fold: 3, ndcg@20 = 0.01417018817482774

training took :  3602.964293718338 s

FOLD 4

done training parameter:  0
easer recall@5,10,20 = 0.011645724793135151,0.01624272142200429,0.02359791602819491
easer ndcg@5,10,20 = 0.008147402945552187,0.009621260748028788,0.01146182221732939

fold took :  931.1311609745026 s
done training parameter:  100
easer recall@5,10,20 = 0.013484523444682808,0.02053325160894882,0.02804167943610175
easer ndcg@5,10,20 = 0.009330889110253199,0.01159280919038863,0.013485896771463092

fold took :  898.1113259792328 s
done training parameter:  200
easer recall@5,10,20 = 0.013637756665645111,0.020380018387986514,0.02819491265706405
easer ndcg@5,10,20 = 0.009419943592210413,0.01155761151766705,0.01352955157746059

fold took :  926.3229651451111 s
done training parameter:  300
easer recall@5,10,20 = 0.013637756665645111,0.020073551946061908,0.02819491265706405
easer ndcg@5,10,20 = 0.009416036192411492,0.011469662649635653,0.013523420599382942

fold took :  915.0410895347595 s
done fold: 4
Best parameter lambda:  200
easer fold: 4, recall@5 = 0.013637756665645111
easer fold: 4, recall@10 = 0.020380018387986514
easer fold: 4, recall@20 = 0.02819491265706405
easer fold: 4, ndcg@5 = 0.009419943592210413
easer fold: 4, ndcg@10 = 0.01155761151766705
easer fold: 4, ndcg@20 = 0.01352955157746059

training took :  3670.606541633606 s

FOLD 5
Note here the prediction interactions of suers was set to 2 instead of 1, this gave a worse result and from now on we will revert to 1
done training parameter:  0
easer recall@5,10,20 = 0.009193993257738278,0.012718357339871283,0.017391970579221577
easer ndcg@5,10,20 = 0.006600966094399004,0.00770459015014973,0.00888022264598895

fold took :  920.6561713218689 s
done training parameter:  100
easer recall@5,10,20 = 0.011722341403616304,0.0165491878639289,0.024977015016855654
easer ndcg@5,10,20 = 0.008276185732672177,0.009837054820840387,0.011946562351919088

fold took :  924.5054144859314 s
done training parameter:  200
easer recall@5,10,20 = 0.01187557462457861,0.01685565430585351,0.024364082133006434
easer ndcg@5,10,20 = 0.008371006372720596,0.009968024465127588,0.011861670867545419

fold took :  871.7968888282776 s
done training parameter:  300
easer recall@5,10,20 = 0.01187557462457861,0.01685565430585351,0.024440698743487588
easer ndcg@5,10,20 = 0.008366149876517197,0.009961786797342003,0.011871301076438216

fold took :  915.8754227161407 s
done fold: 5
Best parameter lambda:  100
easer fold: 5, recall@5 = 0.011722341403616304
easer fold: 5, recall@10 = 0.0165491878639289
easer fold: 5, recall@20 = 0.024977015016855654
easer fold: 5, ndcg@5 = 0.008276185732672177
easer fold: 5, ndcg@10 = 0.009837054820840387
easer fold: 5, ndcg@20 = 0.011946562351919088

training took :  3632.838921070099 s

FOLD6
done training parameter:  0
easer recall@5,10,20 = 0.010573092246399018,0.013178057002758198,0.02022678516702421
easer ndcg@5,10,20 = 0.007883976945083871,0.008735419029246818,0.010480967947869789

fold took :  832.3182265758514 s
done training parameter:  100
easer recall@5,10,20 = 0.013024823781795893,0.01930738584125038,0.028961078761875574
easer ndcg@5,10,20 = 0.00947594769400772,0.011494904532640389,0.013914003857117786

fold took :  941.8775851726532 s
done training parameter:  200
easer recall@5,10,20 = 0.013178057002758198,0.01946061906221269,0.028961078761875574
easer ndcg@5,10,20 = 0.009528511141202891,0.01154927713925987,0.013948666208313448

fold took :  1001.6085674762726 s
done training parameter:  300
easer recall@5,10,20 = 0.013331290223720503,0.01946061906221269,0.028961078761875574
easer ndcg@5,10,20 = 0.009555289183558132,0.011525503579116267,0.013934515074098484

fold took :  907.133134841919 s
done fold: 6
Best parameter lambda:  100
easer fold: 6, recall@5 = 0.013024823781795893
easer fold: 6, recall@10 = 0.01930738584125038
easer fold: 6, recall@20 = 0.028961078761875574
easer fold: 6, ndcg@5 = 0.00947594769400772
easer fold: 6, ndcg@10 = 0.011494904532640389
easer fold: 6, ndcg@20 = 0.013914003857117786

training took :  3682.937514066696 s

FOLD7

done training parameter:  0
easer recall@5,10,20 = 0.008887526815813668,0.013331290223720503,0.018081520073551946
easer ndcg@5,10,20 = 0.006282842924577514,0.007715209207876388,0.008903810569314058

fold took :  469.045045375824 s
done training parameter:  100
easer recall@5,10,20 = 0.011645724793135151,0.017928286852589643,0.028348145878026355
easer ndcg@5,10,20 = 0.00800771124776226,0.010052166525977374,0.012680101442395083

fold took :  450.32097578048706 s
done training parameter:  200
easer recall@5,10,20 = 0.011645724793135151,0.017928286852589643,0.02850137909898866
easer ndcg@5,10,20 = 0.007811271737542226,0.009855049098707458,0.012523474218157176

fold took :  440.4013714790344 s
done training parameter:  300
easer recall@5,10,20 = 0.011645724793135151,0.018081520073551946,0.02850137909898866
easer ndcg@5,10,20 = 0.007776678895589143,0.009872300990214442,0.012502677888070078

fold took :  422.18449115753174 s
done fold: 7
Best parameter lambda:  300
easer fold: 7, recall@5 = 0.011645724793135151
easer fold: 7, recall@10 = 0.018081520073551946
easer fold: 7, ndcg@100 = 0.02850137909898866

FOLD8

done training parameter:  0
easer recall@5,10,20 = 0.012103569787038455,0.015320974413972729,0.021755783667841276
easer ndcg@5,10,20 = 0.008644278259590493,0.009675085134115328,0.011277294513405048

fold took :  894.4307734966278 s
done training parameter:  100
easer recall@5,10,20 = 0.014248506204994637,0.020223686226444004,0.027424544201011185
easer ndcg@5,10,20 = 0.010809357354506463,0.012720577935704337,0.014564674191844075

fold took :  940.6201667785645 s
done training parameter:  200
easer recall@5,10,20 = 0.014401715949134366,0.019917266738164546,0.027884173433430366
easer ndcg@5,10,20 = 0.010813896755170194,0.01258393857423795,0.014619135518528746

fold took :  888.111997127533 s
done training parameter:  300
easer recall@5,10,20 = 0.01409529646085491,0.020223686226444004,0.027884173433430366
easer ndcg@5,10,20 = 0.010608131617067588,0.012593355078560478,0.014548943841700007

fold took :  873.506623506546 s
done fold: 8
Best parameter lambda:  200
easer fold: 8, recall@5 = 0.014401715949134366
easer fold: 8, recall@10 = 0.019917266738164546
easer fold: 8, recall@20 = 0.027884173433430366
easer fold: 8, ndcg@5 = 0.010813896755170194
easer fold: 8, ndcg@10 = 0.01258393857423795
easer fold: 8, ndcg@20 = 0.014619135518528746

training took :  3596.671446800232 s


FOLD 9

done training parameter:  0
easer recall@5,10,20 = 0.008731617647058824,0.011948529411764705,0.016237745098039217
easer ndcg@5,10,20 = 0.006870965396807674,0.007897044263861886,0.008982803533343919

fold took :  438.16470289230347 s
done training parameter:  100
easer recall@5,10,20 = 0.010569852941176471,0.016544117647058824,0.02389705882352941
easer ndcg@5,10,20 = 0.00830100705215601,0.010189638575674542,0.01203512404218246

fold took :  420.230366230011 s
done training parameter:  200
easer recall@5,10,20 = 0.010569852941176471,0.01639093137254902,0.02420343137254902
easer ndcg@5,10,20 = 0.008394932786341895,0.01024291494462689,0.012206512170970215

fold took :  420.9228138923645 s
done training parameter:  300
easer recall@5,10,20 = 0.010723039215686275,0.01639093137254902,0.02420343137254902
easer ndcg@5,10,20 = 0.00844357392686088,0.01022931859126449,0.01218854141181416

fold took :  420.74029636383057 s
done fold: 9
Best parameter lambda:  300
easer fold: 9, recall@5 = 0.010723039215686275
easer fold: 9, recall@10 = 0.01639093137254902
easer fold: 9, ndcg@100 = 0.02420343137254902

training took :  1700.0581793785095 s

TEST set


easer recall@5,10,20 = 0.01133578431372549,0.01685049019607843,0.023284313725490197
easer ndcg@5,10,20 = 0.009093902193378533,0.010831627504325026,0.012396022178357764

easer recall@5,10,20 = 0.010416666666666666,0.015318627450980392,0.02297794117647059
easer ndcg@5,10,20 = 0.007787631671448894,0.009335723224973402,0.011282821047527064

easer recall@5,10,20 = 0.013786764705882353,0.019301470588235295,0.02665441176470588
easer ndcg@5,10,20 = 0.009536017232963897,0.01125091017610231,0.013161341274603191

easer recall@5,10,20 = 0.013786764705882353,0.02022058823529412,0.02971813725490196
easer ndcg@5,10,20 = 0.009410071607709,0.011495282243506284,0.01387402114937357

easer recall@5,10,20 = 0.011948529411764705,0.019301470588235295,0.027573529411764705
easer ndcg@5,10,20 = 0.008031395137120766,0.010351369737366656,0.012412732800166037

easer recall@5,10,20 = 0.014097456328532026,0.02083971805087343,0.02911431198283788
easer ndcg@5,10,20 = 0.009799673072235704,0.012002325708547154,0.014108104284425035

easer recall@5,10,20 = 0.01256512411890898,0.017775053631627336,0.02911431198283788
easer ndcg@5,10,20 = 0.009298740178792779,0.011001595961808438,0.01382161984776801

easer recall@5,10,20 = 0.014710389212381244,0.02022678516702421,0.02911431198283788
easer ndcg@5,10,20 = 0.010305825689173974,0.01204284496019833,0.014289525721791988

easer recall@5,10,20 = 0.012871590560833588,0.01961385228317499,0.029420778424762487
easer ndcg@5,10,20 = 0.008091043009285523,0.010231661445899725,0.012716086719865679

easer recall@5,10,20 = 0.01225865767698437,0.018694452957401166,0.02850137909898866
easer ndcg@5,10,20 = 0.008522228281669087,0.010568004864677117,0.012985901460944242

mean recall@5,10,20 over 10 folds: 0.012777772770156178,0.018814250914892465,0.027547342680559812
mean ndcg@5,10,20 over 10 folds: 0.008987652807377816,0.010911134582740444,0.013104817648482259

standard deviation recall@5,10,20 over 10 folds: 0.0012719276494463781,0.0016235672226876557,0.002373801419346135
standard deviation ndcg@5,10,20 over 10 folds: 0.0007974440861534764,0.0007948161049534008,0.0008935932237865662


Randomized items on the validation set
easer recall@5,10,20 = 0.011488970588235295,0.015471813725490197,0.020986519607843136
easer ndcg@5,10,20 = 0.007962420474848277,0.00924930849856685,0.01061194544391752

easer recall@5,10,20 = 0.011488970588235295,0.01685049019607843,0.025275735294117647
easer ndcg@5,10,20 = 0.008339578345274979,0.010058689547241989,0.012149701922015324

easer recall@5,10,20 = 0.010723039215686275,0.01685049019607843,0.023284313725490197
easer ndcg@5,10,20 = 0.008060335465340978,0.010024795514690266,0.011641588211124734

easer recall@5,10,20 = 0.010418262601501456,0.01639344262295082,0.023900720085797456
easer ndcg@5,10,20 = 0.007560829241332626,0.009466268996651402,0.011297234336953054

easer recall@5,10,20 = 0.014863622433343548,0.02053325160894882,0.028348145878026355
easer ndcg@5,10,20 = 0.01058444258219468,0.012456223296182959,0.014439239134560257

easer recall@5,10,20 = 0.015783021759117378,0.021605884155684953,0.030340177750536317
easer ndcg@5,10,20 = 0.011612322374739319,0.013492114640051992,0.015689617376702526

easer recall@5,10,20 = 0.012871590560833588,0.016395954642966594,0.024364082133006434
easer ndcg@5,10,20 = 0.009199616869959407,0.010327649647931593,0.012317442960439334

easer recall@5,10,20 = 0.013024823781795893,0.017468587189702726,0.026356114005516396
easer ndcg@5,10,20 = 0.009402965624483634,0.010862692194113443,0.013067764047338212

easer recall@5,10,20 = 0.01409529646085491,0.018691588785046728,0.02650528573617282
easer ndcg@5,10,20 = 0.010607775940553267,0.012070340003626313,0.014027029221141836

easer recall@5,10,20 = 0.012867647058823529,0.01715686274509804,0.02466299019607843
easer ndcg@5,10,20 = 0.009759360431605838,0.011143488964609602,0.01300718433954846

mean recall@5,10,20 over 10 folds: 0.012762524504842717,0.017741836586804574,0.025402408441258517
mean ndcg@5,10,20 over 10 folds: 0.009308964735033301,0.010915157130366641,0.012824874699374126

standard deviation recall@5,10,20 over 10 folds: 0.0016824086184614537,0.0018550826224969998,0.002511318412902894
standard deviation ndcg@5,10,20 over 10 folds: 0.0012732165677383743,0.001309606981445398,0.0014729081369913763